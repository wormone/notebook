<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>A notebook on data analysis in python</title>
    <link>https://wormone.github.io/notebook/index.xml</link>
    <description>Recent content on A notebook on data analysis in python</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 17 Apr 2017 14:53:16 +0800</lastBuildDate>
    <atom:link href="https://wormone.github.io/notebook/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Receiver Operating Characteristic (ROC)</title>
      <link>https://wormone.github.io/notebook/post/plot_roc/</link>
      <pubDate>Mon, 17 Apr 2017 14:53:16 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/plot_roc/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Example of Receiver Operating Characteristic (ROC) metric to evaluate
classifier output quality.&lt;/p&gt;

&lt;p&gt;ROC curves typically feature true positive rate on the Y axis, and false
positive rate on the X axis. This means that the top left corner of the plot is
the &amp;ldquo;ideal&amp;rdquo; point - a false positive rate of zero, and a true positive rate of
one. This is not very realistic, but it does mean that a larger area under the
curve (AUC) is usually better.&lt;/p&gt;

&lt;p&gt;The &amp;ldquo;steepness&amp;rdquo; of ROC curves is also important, since it is ideal to maximize
the true positive rate while minimizing the false positive rate.&lt;/p&gt;

&lt;h2 id=&#34;multiclass-settings&#34;&gt;Multiclass settings&lt;/h2&gt;

&lt;p&gt;ROC curves are typically used in binary classification to study the output of
a classifier. In order to extend ROC curve and ROC area to multi-class
or multi-label classification, it is necessary to binarize the output. One ROC
curve can be drawn per label, but one can also draw a ROC curve by considering
each element of the label indicator matrix as a binary prediction
(micro-averaging).&lt;/p&gt;

&lt;p&gt;Another evaluation measure for multi-class classification is
macro-averaging, which gives equal weight to the classification of each
label.&lt;/p&gt;

&lt;p&gt;.. note::&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;See also :func:`sklearn.metrics.roc_auc_score`,
         :ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py`.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(__doc__)

import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle

from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from scipy import interp

# Import some data to play with
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Binarize the output
y = label_binarize(y, classes=[0, 1, 2])
n_classes = y.shape[1]

# Add noisy features to make the problem harder
random_state = np.random.RandomState(0)
n_samples, n_features = X.shape
X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]

# shuffle and split training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,
                                                    random_state=0)

# Learn to predict each class against the other
classifier = OneVsRestClassifier(svm.SVC(kernel=&#39;linear&#39;, probability=True,
                                 random_state=random_state))
y_score = classifier.fit(X_train, y_train).decision_function(X_test)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr[&amp;quot;micro&amp;quot;], tpr[&amp;quot;micro&amp;quot;], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc[&amp;quot;micro&amp;quot;] = auc(fpr[&amp;quot;micro&amp;quot;], tpr[&amp;quot;micro&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Automatically created module for IPython interactive environment
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Plot of a ROC curve for a specific class&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.figure()
lw = 2
plt.plot(fpr[2], tpr[2], color=&#39;darkorange&#39;,
         lw=lw, label=&#39;ROC curve (area = %0.2f)&#39; % roc_auc[2])
plt.plot([0, 1], [0, 1], color=&#39;navy&#39;, lw=lw, linestyle=&#39;--&#39;)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.title(&#39;Receiver operating characteristic example&#39;)
plt.legend(loc=&amp;quot;lower right&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_4_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Plot ROC curves for the multiclass problem&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += interp(all_fpr, fpr[i], tpr[i])

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr[&amp;quot;macro&amp;quot;] = all_fpr
tpr[&amp;quot;macro&amp;quot;] = mean_tpr
roc_auc[&amp;quot;macro&amp;quot;] = auc(fpr[&amp;quot;macro&amp;quot;], tpr[&amp;quot;macro&amp;quot;])

# Plot all ROC curves
plt.figure()
plt.plot(fpr[&amp;quot;micro&amp;quot;], tpr[&amp;quot;micro&amp;quot;],
         label=&#39;micro-average ROC curve (area = {0:0.2f})&#39;
               &#39;&#39;.format(roc_auc[&amp;quot;micro&amp;quot;]),
         color=&#39;deeppink&#39;, linestyle=&#39;:&#39;, linewidth=4)

plt.plot(fpr[&amp;quot;macro&amp;quot;], tpr[&amp;quot;macro&amp;quot;],
         label=&#39;macro-average ROC curve (area = {0:0.2f})&#39;
               &#39;&#39;.format(roc_auc[&amp;quot;macro&amp;quot;]),
         color=&#39;navy&#39;, linestyle=&#39;:&#39;, linewidth=4)

colors = cycle([&#39;aqua&#39;, &#39;darkorange&#39;, &#39;cornflowerblue&#39;])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
             label=&#39;ROC curve of class {0} (area = {1:0.2f})&#39;
             &#39;&#39;.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], &#39;k--&#39;, lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.title(&#39;Some extension of Receiver operating characteristic to multi-class&#39;)
plt.legend(loc=&amp;quot;lower right&amp;quot;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;output_6_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Good to Great Book Review</title>
      <link>https://wormone.github.io/notebook/post/good-to-great/</link>
      <pubDate>Thu, 26 Jan 2017 22:53:16 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/good-to-great/</guid>
      <description>&lt;p&gt;I read &lt;strong&gt;Good to Great in January 2017&lt;/strong&gt;. An awesome read sharing detailed analysis on how good companis became great.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>