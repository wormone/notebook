<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on A notebook on data analysis in python</title>
    <link>https://wormone.github.io/notebook/post/index.xml</link>
    <description>Recent content in Posts on A notebook on data analysis in python</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 05 Sep 2018 13:18:15 +0800</lastBuildDate>
    <atom:link href="https://wormone.github.io/notebook/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>对于AI在气象预报方面应用的认识</title>
      <link>https://wormone.github.io/notebook/post/%E5%AF%B9%E4%BA%8EAI%E5%9C%A8%E6%B0%94%E8%B1%A1%E9%A2%84%E6%8A%A5%E6%96%B9%E9%9D%A2%E5%BA%94%E7%94%A8%E7%9A%84%E8%AE%A4%E8%AF%86/</link>
      <pubDate>Wed, 05 Sep 2018 13:18:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E5%AF%B9%E4%BA%8EAI%E5%9C%A8%E6%B0%94%E8%B1%A1%E9%A2%84%E6%8A%A5%E6%96%B9%E9%9D%A2%E5%BA%94%E7%94%A8%E7%9A%84%E8%AE%A4%E8%AF%86/</guid>
      <description>&lt;p&gt;之前在一篇关于 &lt;a href=&#34;https://wormone.github.io/notebook/notebook/post/机器学习与环境气象：空气质量预报：0方案设计/&#34;&gt;机器学习在空气质量预报方面应用&lt;/a&gt; 的总结中涉及到过这个话题。&lt;/p&gt;

&lt;p&gt;现在大数据和 AI 实在是太热门了，气象也希望能借助大数据和 AI 的力量取得新的进步。&lt;/p&gt;

&lt;p&gt;其实，返回头想想，气象本身一直就是大数据，是专业领域的一种大数据，特点包括但不限于：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;多维度。包括空间、时间、要素等。&lt;/li&gt;
&lt;li&gt;多源。来源包括观测站点和探空传感器、卫星遥感、模式预报等等。&lt;/li&gt;
&lt;li&gt;异构和标准化。数据格式包括文本、二进制、图像、netCDF、HDF 等等，但又有很强的标准化。&lt;/li&gt;
&lt;li&gt;质量控制和&amp;rdquo;三性&amp;rdquo;。监测数据质量控制，数据的准确性、及时性、代表性。&lt;/li&gt;
&lt;li&gt;时空数据分辨率、覆盖度或完整性。监测数据的时间和空间分辨率和覆盖范围，预报数据的积累等等。&lt;/li&gt;
&lt;li&gt;海量。数据量实在是太大了。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;AI 在气象预报中能做什么呢？之前的总结中跟图像、语音识别问题的对比主要是从准确率上说的（也提到了在短时临近预报和气候预测方面的适用性），其实 &lt;strong&gt;更重要的一个区别&lt;/strong&gt; 是，图像和语音识别这类问题都是用 AI 从图像和语音属性特点中学习识别模式，而气象预报（常规的天气预报）则不同：它不能从未知的“实况”来提取属性特点进行识别，而只能从数值预报对未来的预报结果中提取特征属性从而对未来进行预报。&lt;/p&gt;

&lt;p&gt;也就是说，AI 用于图像和语音识别这类问题，训练用的是本身（属性 X 来自于目标 Y 本身），而用于天气预报则不一样，是把数值预报的结果（多时空要素，包括目标要素本身）当作目标要素 Y 的属性特征 X，即 X 并不是从 Y 提取获得的。因此，AI 应用于气象预报，这个用法本身和以往的订正算法本质上是一样的，目的或者说用途在于减小数值预报的误差，而不是直接进行预报。&lt;/p&gt;

&lt;p&gt;因为脱离开数值预报，AI 是不能单独进行天气预报的。天气系统的演变是数学物理方程所描述的，单靠 AI 是做不到的。&lt;/p&gt;

&lt;p&gt;数值预报是数学物理方程对天气变化的描述，是现代气象预报的基础。那么 AI 能否替代数值预报，给天气预报带来“质的飞跃”呢？&lt;/p&gt;

&lt;p&gt;如果可以的话，应该早就改变了，但实际上没有。机器学习方法和 AI 相比于传统的动力统计订正方法，在减小数值预报误差方面的表先可能会更好一些，但是没有本质上的改变。比如一个地方气温的预报误差，传统方法能将均方根误差 RMSE 减小到 2.1℃，而用某种机器学习或 AI 算法可以将 RMSE 减小到 1.9℃，而始终无法减小到更小了，似乎到达了极限。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AI 有没有可能发展到能够替代数值预报的水平来做天气预报？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;应该是不行的。数值预报所依赖的数学物理方程是试图对大气的变化进行精确的描述，而机器学习和 AI 则并不是从物理方面进行描述，也不寻求物理方面的解释。目前还没有什么 AI 系统发展到能够达到数值预报的水平，如果可以的话，那么它的复杂程度和计算成本估计会太超乎想象以至于不现实、不可能存在。人创造的 AI 有可能走火入魔，说出一些出格的话、干出一些惊人的事，但是对自然界的预测始终不能超越人类知识的边界。&lt;/p&gt;

&lt;p&gt;其实改进数值预报准确率一直都在演进，集合预报、多模式集成、超级集合，各种集合和集成算法的尝试，统计学、机器学习、人工神经网络，都包括在内。数值预报模式、数据同化的改进应该是基础性的，各种订正的作用是“锦上添花”，没有锦、光有花不行，锦不好、光靠花也不行。&lt;/p&gt;

&lt;p&gt;有一种普遍的看法认为天气预报的误差是可以无限减小的，预报可以达到很准确。其实气象预报是有 &lt;strong&gt;可预报性&lt;/strong&gt; 问题的，也就是总有那么一部分 &lt;strong&gt;不可预报&lt;/strong&gt;。而这种 &lt;strong&gt;可预报性&lt;/strong&gt; 或 &lt;strong&gt;不可预报性&lt;/strong&gt; 在不同的时间和空间上是变化的，在不同的时间或空间尺度上也是变化的。通俗解释一下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;不可预报性的存在源于自然的复杂程度超过了人类的探知和认知能力&lt;/li&gt;
&lt;li&gt;完全准确（梦想预报） = 可预报性（现实中预报的极限） + 不可预报性（人类对自然的敬畏）&lt;/li&gt;
&lt;li&gt;某地夏季和冬季（气团控制相对稳定）的预报误差要比春季和秋季（过渡季节天气多变）大&lt;/li&gt;
&lt;li&gt;另外一个地方（气候不同）的情况却有所不同&lt;/li&gt;
&lt;li&gt;越讲究时间、空间、要素准确的预报（精细化预报）越困难&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;另外，&lt;strong&gt;准确&lt;/strong&gt; 这个概念其实是需要先给出定义，然后才能谈论的。光说“准确”两个字是不行的，一定要有定量化的标准，才能进行计算和比较。而且当需要比较的时候，准确的定义必须一致，也就是有一致的标准。比如气象部门说的气温预报准确，其实指的是预报误差的绝对值在 2℃ 以内，降水预报准确可能说的是预报了暴雨（24h 降水量大于 50 毫米）也确实出现了暴雨（实际下了 100 多毫米）。标准不同，准确与否也不一样。准不准确要看人的需求。&lt;/p&gt;

&lt;p&gt;【参考文献】&lt;/p&gt;

&lt;p&gt;[1] 周秀骥. 大气随机动力学与可预报性[J]. 气象学报, 2005, 63(5):806-811.&lt;/p&gt;

&lt;p&gt;[2] 穆穆, 李建平, 段晚锁, 等. 气候系统可预报性理论研究[J]. 气候与环境研究, 2002, 7(2):227-235.&lt;/p&gt;

&lt;p&gt;[3] Eugenia Kalnay. 大气模式、资料同化和可预报性[M]. 气象出版社, 2005.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>从高德地图获取最新行政区划边界数据</title>
      <link>https://wormone.github.io/notebook/post/%E4%BB%8E%E9%AB%98%E5%BE%B7%E5%9C%B0%E5%9B%BE%E8%8E%B7%E5%8F%96%E6%9C%80%E6%96%B0%E8%A1%8C%E6%94%BF%E5%8C%BA%E5%88%92%E8%BE%B9%E7%95%8C%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Tue, 28 Aug 2018 09:18:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E4%BB%8E%E9%AB%98%E5%BE%B7%E5%9C%B0%E5%9B%BE%E8%8E%B7%E5%8F%96%E6%9C%80%E6%96%B0%E8%A1%8C%E6%94%BF%E5%8C%BA%E5%88%92%E8%BE%B9%E7%95%8C%E6%95%B0%E6%8D%AE/</guid>
      <description>&lt;p&gt;行政区划边界数据在很多地方都会用到。然而，国内公开的数据一般跟不上最新的变更，国外公开的数据因为问题很多（国界线、南海、台湾等等）所以最好别用。&lt;/p&gt;

&lt;p&gt;另外，一般人可能没有注意到，很多软件自带的中国国界数据也是有问题的，最好别用，即便仅仅是作为 GIS 底图。&lt;/p&gt;

&lt;p&gt;不少文献中的底图，中国的行政边界都有问题。&lt;/p&gt;

&lt;p&gt;有没有办法获取 &lt;strong&gt;正确&lt;/strong&gt; 且 &lt;strong&gt;精细&lt;/strong&gt; 且 &lt;strong&gt;更新及时&lt;/strong&gt; 的行政区划边界数据呢？几经搜寻，发现高德地图满足要求。&lt;/p&gt;

&lt;p&gt;现在我们需要从高德的 API 里面查出 country、province、city、district，也就是国、省、市、区县四个级别的行政区划边界。
当然，还有乡镇一级的行政区划边界，有需要的可以参照以下的方法自行处理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;amap_CN.png&#34; alt=&#34;amap_CN.png&#34; title=&#34;amap_CN.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;用浏览器在找到的连接处点击鼠标右键，在弹出的菜单中找到 Copy 下的 Copy as cURL(bash)，然后粘贴进 shell 命令行回车，我们就看到了原始数据的样子：
返回的是 jsonp，里面包含了一段 json 数据，看到 &lt;code&gt;level: &amp;quot;country&amp;quot;, name: &amp;quot;中华人民共和国&amp;quot;, polyline: &amp;quot;120.823872,40.530257;...&amp;quot;&lt;/code&gt;，
显然，&lt;code&gt;polyline&lt;/code&gt; 就是边界数据，再仔细看，&lt;code&gt;;&lt;/code&gt; 分隔的是经纬度点，&lt;code&gt;|&lt;/code&gt; 分割的是不同的多边形。&lt;/p&gt;

&lt;p&gt;找到了？是的，我们找到了行政区划边界数据的 API，下面就好办了。&lt;/p&gt;

&lt;p&gt;从国家到区县，很多行政边界并不仅仅是一个多边形，而是由多个多边形组成的，这一点要特别注意。&lt;/p&gt;

&lt;p&gt;接下来我们可以编写一段程序，自动查询任意行政边界，并利用 &lt;code&gt;pyshp&lt;/code&gt; 转换成 shp 格式保存到本地：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import re
import json
import requests
import shapefile
from xpinyin import Pinyin  # 便于转换中文名称为拼音


BASE_URL = u&#39;http://restapi.amap.com/v3/config/district?subdistrict=1&amp;amp;extensions=all&amp;amp;level=district&amp;amp;key=608d75903d29ad471362f8c58c550daf&amp;amp;s=rsv3&amp;amp;output=json&amp;amp;keywords={name}&amp;amp;callback=jsonp_923427_&amp;amp;platform=JS&amp;amp;logversion=2.0&amp;amp;sdkversion=1.3&amp;amp;appname=http%3A%2F%2Flbs.amap.com%2Fapi%2Fjavascript-api%2Fexample%2Fdistrict-search%2Fdraw-district-boundaries%2F&amp;amp;csid=7EB50235-A6C0-45C3-B08C-2F77FE1837AC&#39;  # .format(name=&amp;quot;中国&amp;quot;)

HEADERS = {
    &#39;Accept-Encoding&#39;: &#39;gzip, deflate, sdch&#39;,
    &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.8&#39;,
    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36&#39;,
    &#39;Accept&#39;: &#39;*/*&#39;,
    &#39;Referer&#39;: &#39;http://lbs.amap.com/api/javascript-api/example/district-search/draw-district-boundaries/&#39;,
    &#39;Cookie&#39;: &#39;guid=70c9-87fe-5807-3f17; cna=YKn+D5QKFQ0CAd3vO4CwxB9s; l=Ara234AMAU3gNSreb6UFtrzXhua4gPoX; isg=AqmphLSq2mevHubTbJlaWcpIuFVMnZ2o89H-lkuechDJEskkk8ateJdKogHf; key=608d75903d29ad471362f8c58c550daf&#39;,
    &#39;Connection&#39;: &#39;keep-alive&#39;,
}


def get_bdr_by_name_allpoly(name):
    out_name = Pinyin().get_pinyin(name, &#39;&#39;)
    url = BASE_URL.format(name=name)
    r = requests.get(url=url, headers=HEADERS)
    if r.status_code == requests.codes.ok:
        js = r.text
    else:
        print(&#39;Fail with status_code = {}&#39;.format(r.status_code))
    with open(out_name, &#39;w&#39;) as o:
        o.write(js.encode(&amp;quot;utf-8&amp;quot;))
    m = re.findall(r&amp;quot;jsonp_\d{1,10}_\((?P&amp;lt;json&amp;gt;.*)\)&amp;quot;, js)
    js = m[0]
    js = json.loads(js)
    polylines = js.get(&#39;districts&#39;)[0].get(&#39;polyline&#39;)
    polylines = polylines.split(&#39;|&#39;)
    poly_list = []
    w = shapefile.Writer(shapefile.POLYGON)
    w.field(b&#39;NAME&#39;, b&#39;C&#39;)
    for polyline in polylines:
        polyline = polyline.split(&#39;;&#39;)
        x, y = [[float(p.split(&#39;,&#39;)[i]) for p in polyline] for i in (0, 1)]
        polygon = [[x[j], y[j]] for j in range(len(x))]
        w.poly(parts=[polygon])
        w.record(b&#39;part&#39;)
        poly_list.append(polygon)
    w.save(b&#39;{out_name}&#39;.format(out_name=out_name))
    return poly_list


if __name__ == &#39;__main__&#39;:
    get_bdr_by_name_allpoly(name=u&#39;中国&#39;)
    get_bdr_by_name_allpoly(name=u&#39;河北省&#39;)
    get_bdr_by_name_allpoly(name=u&#39;武汉市&#39;)
    get_bdr_by_name_allpoly(name=u&#39;若羌县&#39;)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们用 ArcGIS Earth 打开看一下（别的 GIS 软件一样的）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;amap_shp_view.png&#34; alt=&#34;amap_shp_view.png&#34; title=&#34;amap_shp_view.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然，还可以另外写为 geojson 或 kml 等格式，不难。&lt;/p&gt;

&lt;p&gt;最后，需要 &lt;strong&gt;特别说明&lt;/strong&gt; 的是，由于高德地图在国内采用的是 GCJ-02 坐标系（是在 WGS84 坐标上进行的随机加密，有几十到几百米的偏移），因此，如果对精度要求非常高，就必须经过相应的坐标转换（可自行从网上搜寻坐标转换方法），如果不在意，可以忽略。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>tornado日志默认格式变更恢复</title>
      <link>https://wormone.github.io/notebook/post/tornado%E6%97%A5%E5%BF%97%E9%BB%98%E8%AE%A4%E6%A0%BC%E5%BC%8F%E5%8F%98%E6%9B%B4%E6%81%A2%E5%A4%8D/</link>
      <pubDate>Mon, 27 Aug 2018 12:18:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/tornado%E6%97%A5%E5%BF%97%E9%BB%98%E8%AE%A4%E6%A0%BC%E5%BC%8F%E5%8F%98%E6%9B%B4%E6%81%A2%E5%A4%8D/</guid>
      <description>&lt;p&gt;我们知道，tornado 日志默认格式是类似这样的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[I 180811 19:43:13 web:2106] 200 GET / (127.0.0.1) 4.25ms
[W 180811 19:43:14 web:2106] 404 GET /favicon.ico (127.0.0.1) 2.01ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;[级别 Infor/Warning/Error 时间 模块:行数] 状态码 方法 路由 耗时&lt;/code&gt;，记录既简短又实用，非常好。&lt;/p&gt;

&lt;p&gt;但是有一天我发现自己的 tornado 日志格式 &lt;strong&gt;突变&lt;/strong&gt; 了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;INFO:tornado.access:200 GET / (127.0.0.1) 7.65ms
INFO:tornado.access:200 GET /static/images/favicon.ico (127.0.0.1) 1.42ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这是怎么回事呢？关键是时间没了，这个不好！
猜测是自己新引入的什么模块本身引入了 logging 模块并且有所设置，
从而改变了 tornado 本身对于 logging 的全局设置。&lt;/p&gt;

&lt;p&gt;logging 这个模块比较 &lt;strong&gt;特殊&lt;/strong&gt;，即便在被引用的模块中，也会改变主程序中的 logging 设置！&lt;/p&gt;

&lt;p&gt;首先找到嫌疑的模块有哪些，然后逐一试验一下是否引入，查看 tornado 日志格式。
最终发现模块 &lt;code&gt;metpy.gridding&lt;/code&gt; 引入并设置了 logging。&lt;/p&gt;

&lt;p&gt;为了恢复 tornado 日志的默认格式，需要在 tornado 主程序开头所有 &lt;code&gt;import&lt;/code&gt; 语句之后增加如下语句：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import logging
from tornado.log import LogFormatter
root_logger = logging.getLogger()
root_streamhandler = root_logger.handlers[0]
root_streamhandler.setFormatter(LogFormatter())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;好了，这样就可以了，&lt;code&gt;LogFormatter()&lt;/code&gt; 是 tornado 默认的日志格式。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>scatter散点图大小自适应</title>
      <link>https://wormone.github.io/notebook/post/scatter%E6%95%A3%E7%82%B9%E5%9B%BE%E5%A4%A7%E5%B0%8F%E8%87%AA%E9%80%82%E5%BA%94/</link>
      <pubDate>Mon, 27 Aug 2018 10:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/scatter%E6%95%A3%E7%82%B9%E5%9B%BE%E5%A4%A7%E5%B0%8F%E8%87%AA%E9%80%82%E5%BA%94/</guid>
      <description>&lt;p&gt;我们知道，scatter 散点图的基本绘图参数包括几个方面：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;点的位置坐标 x 和 y&lt;/li&gt;
&lt;li&gt;点的颜色 c&lt;/li&gt;
&lt;li&gt;点的大小 s&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;至于点的标记样式，以及标记外边线宽度、颜色和整体透明度、colorbar 等等，这些就不讨论了，按照自己的需要设置即可。&lt;/p&gt;

&lt;p&gt;今天我们主要关注一下 marker 大小的设置问题。来看一个例子：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plot


x, y, z = [np.random.randint(-1000, 1000, (10, 10)) for _ in range(3)]

plt.scatter(x, y, c=z, s=np.abs(z))
cbar = plt.colorbar()
plt.axist(&#39;scaled&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;scatter_plot_0.png&#34; alt=&#34;scatter_plot_0.png&#34; title=&#34;scatter_plot_0.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;大小 s 必须为正，不能为负，因此这里用的是 &lt;code&gt;s=np.abs(z)&lt;/code&gt;。图本质上没有问题，只是点的大小看上去不太合适。
我们来修正一下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;s = 15 * (np.abs(z) / np.mean(np.abs(z)))**2
plt.scatter(x, y, c=z, s=s)
cbar = plt.colorbar()
plt.axist(&#39;scaled&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;scatter_plot_1.png&#34; alt=&#34;scatter_plot_1.png&#34; title=&#34;scatter_plot_1.png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;上述的处理其实只是用 z 的绝对值与 z 绝对值均值的比值的平方对基准大小（这里是 15）做了调节。
注意，再控制 marker 大小时，绝对值是必须的，这样几乎完全可以避免均值为 0 情况的出现（除非 z 本身全部为 0）。&lt;/p&gt;

&lt;p&gt;matplotlib 官网上的例子非常丰富，绝大多数情况查查例子自己试验一下就好了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>python调用外部程序：subprocess的一个应用实例</title>
      <link>https://wormone.github.io/notebook/post/python%E8%B0%83%E7%94%A8%E5%A4%96%E9%83%A8%E7%A8%8B%E5%BA%8F%EF%BC%9Asubprocess%E7%9A%84%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B/</link>
      <pubDate>Mon, 27 Aug 2018 09:00:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/python%E8%B0%83%E7%94%A8%E5%A4%96%E9%83%A8%E7%A8%8B%E5%BA%8F%EF%BC%9Asubprocess%E7%9A%84%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%E5%AE%9E%E4%BE%8B/</guid>
      <description>&lt;p&gt;这次我们要解决的问题是这样的：&lt;/p&gt;

&lt;p&gt;有一个算法模块或者模型本身是由其他语言编写并且编译好的二进制可执行程序，
我们现在需要用 python 调用执行该程序，并且自动获知该程序的执行状态，包括可能输出的错误信息。&lt;/p&gt;

&lt;p&gt;如果该程序遇到错误可以直接输出错误信息并且退出，这种情况显然还是非常容易处理的，
我们只需要将其输出的错误信息重定向到一个日志文件中即可，&lt;code&gt;os.system()&lt;/code&gt; 足以解决问题：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os

if os.name == &#39;posix&#39;:
    os.system(&#39;nohup ./main.exe &amp;gt; log 2&amp;gt;&amp;amp;1 &amp;amp;&#39;)
if os.name == &#39;nt&#39;:
    os.system(&#39;main.exe 1&amp;gt;log 2&amp;gt;&amp;amp;1&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这次的情况是，该程序遇到错误时首先输出了一些错误信息提示，说必须手动按下 &lt;code&gt;&amp;lt;Enter&amp;gt;&lt;/code&gt; 键才能退出。
用 &lt;code&gt;os.system()&lt;/code&gt; 显然不方便获取错误信息内容去判断是否要额外执行其他操作。
这时用 &lt;code&gt;subprocess.Popen()&lt;/code&gt; 就好了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import os
import subprocess

# 比如应用程序所在路径是 bin_dir
os.chdir(bin_dir)

if os.name == &#39;posix&#39;:
    p = subprocess.Popen(&#39;./main.exe&#39;, 
        stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)
if os.name == &#39;nt&#39;:
    p = subprocess.Popen(&#39;main.exe&#39;, 
        stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)
# 一行一行读取 stdout，因为当进程 p 没有结束时，stdout 没有文件结束符，所以不能被 read()
stdout = &#39;&#39;
while p.poll() is None:
    line = p.stdout.readline()
    stdout += line
    if &#39;&amp;lt;Enter&amp;gt;&#39; in line:
        subprocess.Popen.kill(p)  # 读到提示按 &#39;&amp;lt;Enter&amp;gt;&#39; 键的信息时，结束程序，退出
        break
else:
    stdout = p.stdout.read()
# print(stdout)
# 可以写出 stdout 到一个日志文件中
with open(&#39;log&#39;, &#39;w&#39;) as f:
    f.write(stdout)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;好了，就到这里。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>向量（直线）与栅格图像的交点（像元）计算</title>
      <link>https://wormone.github.io/notebook/post/%E5%90%91%E9%87%8F%EF%BC%88%E7%9B%B4%E7%BA%BF%EF%BC%89%E4%B8%8E%E6%A0%85%E6%A0%BC%E5%9B%BE%E5%83%8F%E7%9A%84%E4%BA%A4%E7%82%B9%EF%BC%88%E5%83%8F%E5%85%83%EF%BC%89%E8%AE%A1%E7%AE%97/</link>
      <pubDate>Tue, 27 Feb 2018 22:35:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E5%90%91%E9%87%8F%EF%BC%88%E7%9B%B4%E7%BA%BF%EF%BC%89%E4%B8%8E%E6%A0%85%E6%A0%BC%E5%9B%BE%E5%83%8F%E7%9A%84%E4%BA%A4%E7%82%B9%EF%BC%88%E5%83%8F%E5%85%83%EF%BC%89%E8%AE%A1%E7%AE%97/</guid>
      <description>&lt;p&gt;这次还是一个卫星项目，问题是这样的：&lt;/p&gt;

&lt;p&gt;在一个栅格图像上，有一个点的位置上有一个向量，需要计算的是这个向量所在的直线所经过的像元有哪些。&lt;/p&gt;

&lt;p&gt;问题看上去简单，但似乎实现起来还有点麻烦。&lt;/p&gt;

&lt;p&gt;还好我们有 PIL 可以用，问题就简化成只要找到这条直线的两个&lt;strong&gt;端点&lt;/strong&gt;就行了，思路是这样的：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;这两个端点必然在栅格图像最外圈上，只要找到这上面的两个点就行了；&lt;/li&gt;
&lt;li&gt;首先提取最外圈所有像素的坐标，然后计算每个点与给定点之间的向量，计算该向量与所给向量之前&lt;strong&gt;相互平行&lt;/strong&gt;的一个度量；&lt;/li&gt;
&lt;li&gt;找到最为相互平行的两个度量所对应的点，那就基本大功告成了。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;最后利用 PIL，把找到的两个点在栅格图像上画出直线，找到直线经过的像素点就完成了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw


def vector_across_image(p=(10, 30), u=-1, v=2, size=100, isPlot=False):
    &amp;quot;&amp;quot;&amp;quot;vector line across a square image
    
    A point on the image with a direction like u,v components, make a vector,
    which across the image, result as a line of pixels
    
    Keyword Arguments:
        p {tuple} -- position of the point on the image (default: {(10, 30)})
        u {number} -- u component (default: {-1})
        v {number} -- v component (default: {2})
        size {number} -- image size (default: {100})
        isPlot {bool} -- view the result or not (default: {False})
    
    Returns:
        points -- pixels on the line
    &amp;quot;&amp;quot;&amp;quot;
    # make a square
    x, y = np.mgrid[:size, :size]
    # take the outline of the square
    outline = zip(x[0, :], y[0, :]) + zip(x[-1, :], y[-1, :]) \
            + zip(x[:, 0], y[:, 0]) + zip(x[:, -1], y[:, -1])
    outline = np.array(outline)

    # find the two interception points on the outline, i.e. p0 and p1
    # delta = [(_[0]-p[0])*v - (_[1]-p[1])*u for _ in outline]
    delta = (outline[:, 0] - p[0]) * v - (outline[:, 1] -p[1]) * u
    delta = np.abs(delta)
    ascding = np.argsort(delta)
    p0 = outline[ascding[0]]
    p1 = outline[ascding[1]]

    # get the line using PIL
    image = Image.new(&#39;1&#39;, (size, size))
    draw = ImageDraw.Draw(image)
    draw.line((p0[0], p0[1], p1[0], p1[1]), fill=&#39;white&#39;)
    color = list(image.getdata())
    color = np.reshape(color, (size, size))
    px = y[color==255].ravel()  # !!! x--&amp;gt;y
    py = x[color==255].ravel()  # !!! y--&amp;gt;x
    points = zip(px, py)

    if isPlot:
        plt.subplot(211)
        plt.plot(px, py, &#39;rs&#39;)
        plt.plot(p[0], p[1], &#39;ko&#39;, ms=20)
        plt.plot([p0[0], p1[0]], [p0[1], p1[1]], &#39;b-&#39;)
        plt.axis(&#39;scaled&#39;)
        plt.axis([0, size, 0, size])
        plt.subplot(212)
        plt.imshow(color)
        plt.show()

    return points, color


def test():
    points, color = vector_across_image(p=(10, 30), u=-1, v=2, size=100, isPlot=True)
    # print color


if __name__ == &#39;__main__&#39;:
    test()
    # from timeit import timeit
    # t = timeit(&#39;vector_across_image()&#39;, &#39;from __main__ import vector_across_image&#39;, number=140*140)
    # print(t)

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>中圆点算法（Midpoint circle algorithm）的简单实现</title>
      <link>https://wormone.github.io/notebook/post/%E4%B8%AD%E5%9C%86%E7%82%B9%E7%AE%97%E6%B3%95%EF%BC%88Midpoint%20circle%20algorithm%EF%BC%89%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Mon, 22 Jan 2018 14:35:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E4%B8%AD%E5%9C%86%E7%82%B9%E7%AE%97%E6%B3%95%EF%BC%88Midpoint%20circle%20algorithm%EF%BC%89%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/</guid>
      <description>&lt;p&gt;最近再做一个卫星遥感监测台风的项目，其中有个算法要提取台风中心一定半径的一圈像素然后做一些统计。查了查，原来这个算法还有个名字叫做&lt;a href=&#34;https://en.wikipedia.org/wiki/Midpoint_circle_algorithm&#34;&gt;&lt;strong&gt;中圆点算法（Midpoint circle algorithm）&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;中圆点算法，说简单点就是设定一个半径画个圆，然后看看哪些像素在这个圆圈上。&lt;/p&gt;

&lt;p&gt;其实利用 PIL 可以很简单的实现中圆点算法的功能，找出哪些像素组成了圆圈。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;midpoint_circle_PIL.png&#34; alt=&#34;midpoint_circle_PIL.tif&#34; title=&#34;midpoint_circle_PIL.tif&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw


def get_midpoint_circle_pixels(size=100, radius=30, isPlot=False):
    &#39;&#39;&#39;get midpoint circle pixels easily with PIL
    
    Keyword Arguments:
        size {number} -- image size (default: {100})
        radius {number} -- circle radius, no more than size/2 (default: {30})
        isPlot {bool} -- show image or not (default: {False})
    
    Returns:
        x -- x coordinate
        y -- y coordinate
        points -- [[x[i], y[i]] for i in range(len(x))]
    &#39;&#39;&#39;

    # create new image, size x size pixels, 1 bit per pixel
    image = Image.new(&#39;1&#39;, (size, size))
    draw = ImageDraw.Draw(image)

    # actually a circle was drawn
    lower_left = size/2 - radius
    upper_right = size/2 + radius
    draw.ellipse((lower_left, lower_left, upper_right, upper_right), outline=&#39;white&#39;)

    # image.show()

    # get pixel values
    color = list(image.getdata())
    color = np.reshape(color, (size, size))

    # get pixel coordinate with color=&#39;white&#39;
    grid_x, grid_y = np.mgrid[:size, :size]
    x = grid_x[color==255].ravel()
    y = grid_y[color==255].ravel()
    points = [[x[_], y[_]] for _ in range(len(x))]

    if isPlot:
        plt.pcolor(color)
        plt.axis(&#39;scaled&#39;)
        plt.hold(True)
        plt.plot(x, y, &#39;wo&#39;)
        plt.show()

    return x, y, points


if __name__ == &#39;__main__&#39;:
    size = 100
    x1, y1, _ = get_midpoint_circle_pixels(size=size, radius=10)
    x2, y2, _ = get_midpoint_circle_pixels(size=size, radius=25)
    x3, y3, _ = get_midpoint_circle_pixels(size=size, radius=40)
    plt.hold(True)
    plt.plot(x1, y1, &#39;ro&#39;)
    plt.plot(x2, y2, &#39;go&#39;)
    plt.plot(x3, y3, &#39;bo&#39;)
    plt.axis(&#39;scaled&#39;)
    plt.xlim(0, size)
    plt.ylim(0, size)
    plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用上面的方法，就得到的一组坐标点，但是这些点并没有按照画圆的方向排列……&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x, y, points = get_midpoint_circle_pixels(size=10, radius=3, isPlot=True)
print x
print y
print points
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[2 2 2 2 2 3 3 3 3 4 4 5 5 6 6 7 7 7 7 8 8 8 8 8]
[3 4 5 6 7 2 3 7 8 2 8 2 8 2 8 2 3 7 8 3 4 5 6 7]
[[2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 2], [3, 3], [3, 7], [3, 8], [4, 2], [4, 8], [5, 2], [5, 8], [6, 2], [6, 8], [7, 2], [7, 3], [7, 7], [7, 8], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;midpoint_circle_points.png&#34; alt=&#34;midpoint_circle_points.tif&#34; title=&#34;midpoint_circle_points.tif&#34; /&gt;&lt;/p&gt;

&lt;p&gt;不过排列起来也并不困难，做法是这样的：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;先预定输出排列好的列表为 ring；&lt;/li&gt;
&lt;li&gt;从 points 里找到 &lt;strong&gt;center top&lt;/strong&gt; 的那一个点，放入 ring；&lt;/li&gt;
&lt;li&gt;然后按照顺时针方向找到下一个点追加到 ring 里面，直到结束；

&lt;ul&gt;
&lt;li&gt;这个点必然在上一个点周围的 8 个点之中；&lt;/li&gt;
&lt;li&gt;按照先&lt;strong&gt;上/右/下/左&lt;/strong&gt;然后&lt;strong&gt;右上/右下/左下/左上&lt;/strong&gt;顺序搜索；&lt;/li&gt;
&lt;li&gt;只要从这 8 个点里面找到一个点，当它在 points 里但是尚未在 ring 里，那就找到了；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特别注意&lt;/strong&gt;，第一次搜索时，只需要找到唯一一个 next 点，这才能确保 ring 的排序方向。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;# sort points clockwise
ymax = max(y)
xmid = int(np.median(x))
first_point = [xmid, ymax]
ring = [first_point,]
for _ in range(len(points)):
    x, y = ring[-1]
    # search in the 8 points around: firstly top/right/bottom/left and secondly ur/lr/ll/ul
    possible = [
        [x, y+1], [x+1, y], [x, y-1], [x-1, y],
        [x+1, y+1], [x+1, y-1], [x-1, y-1], [x-1, y+1]]
    for p in possible:
        if (p in points) and (p not in ring):
            ring.append(p)
            # at the first point, find the only one next point: this will insure the direction
            if len(ring) == 2:
                break
x = [ring[_][0] for _ in range(len(ring))]
y = [ring[_][1] for _ in range(len(ring))]
points = [[x[_], y[_]] for _ in range(len(x))]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就完成了 x, y, points 的排列。这几句话可以追加到 &lt;code&gt;get_midpoint_circle_pixels()&lt;/code&gt; 方法里，直接输出排序结果。最后画个图出来验证一下排序结果是否正确：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;size = 50
for r in range(1, 23):
    x, y, points = get_midpoint_circle_pixels(size=size, radius=r)
    plt.plot(x, y, &#39;-s&#39;, label=str(r))
    plt.axis(&#39;scaled&#39;)
    plt.xlim(0, size)
    plt.ylim(0, size)
    # plt.legend(loc=&#39;best&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;midpoint_circle_ring.png&#34; alt=&#34;midpoint_circle_ring.tif&#34; title=&#34;midpoint_circle_ring.tif&#34; /&gt;&lt;/p&gt;

&lt;p&gt;结果是正确的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GeoTiff图像拼接</title>
      <link>https://wormone.github.io/notebook/post/GeoTiff%E5%9B%BE%E5%83%8F%E6%8B%BC%E6%8E%A5/</link>
      <pubDate>Fri, 29 Dec 2017 09:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/GeoTiff%E5%9B%BE%E5%83%8F%E6%8B%BC%E6%8E%A5/</guid>
      <description>&lt;p&gt;&lt;strong&gt;GeoTiff&lt;/strong&gt; 作为 TIFF 的一种扩展，在 TIFF 的基础上定义了一些地理标签， 来对各种坐标系统、椭球基准、投影信息等进行定义和存储，使图像数据和地理数据存储在同一图像文件中。&lt;/p&gt;

&lt;p&gt;今天我们要用到的数据是 ASTER 数字高程地形数据，可以在 &lt;a href=&#34;https://earthexplorer.usgs.gov/&#34;&gt;USGS Earth Explorer site&lt;/a&gt; 注册后，自定义区域下载。&lt;/p&gt;

&lt;p&gt;ASTER 每幅图像是范围为 1° × 1° 且分辨率为 1 arc-second (~30m) 的 GeoTiff 格式文件。其&lt;strong&gt;命名方式&lt;/strong&gt;类似于：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ASTGTM2_N26E116_dem.tif&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其中 &lt;code&gt;N26E116&lt;/code&gt; 表示左下角的经纬度坐标。这幅图像的&lt;strong&gt;地理变换信息&lt;/strong&gt;信息如下：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(115.99986111111112, 0.0002777777777777778, 0.0, 27.000138888888888, 0.0, -0.0002777777777777778)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其对应的分别是：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x0, dx, dxdy, y0, dydx, dy&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;x0 即最小经度，dx 和 dy 是经纬度方向的变化率（数值等于经纬度范围除以格点数），&lt;strong&gt;注意&lt;/strong&gt;这里 dy 为负值，因此 y0 为最大纬度。如果 dy 为正，则 y0 应为最小纬度。&lt;/p&gt;

&lt;p&gt;这幅图像的&lt;strong&gt;投影信息&lt;/strong&gt;如下（其实 ASTER 同一数据源的所有图像的投影信息都是一样的）：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;GEOGCS[&amp;quot;WGS 84&amp;quot;,DATUM[&amp;quot;WGS_1984&amp;quot;,SPHEROID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;7030&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;6326&amp;quot;]],PRIMEM[&amp;quot;Greenwich&amp;quot;,0],UNIT[&amp;quot;degree&amp;quot;,0.0174532925199433],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;4326&amp;quot;]]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;我们今天要做的是将若干幅数字图像拼接起来，形成一个更大的图像。简单来说就是像拼积木一样拼起来，道理很简单。&lt;/p&gt;

&lt;p&gt;我们利用的工具是 &lt;strong&gt;&lt;a href=&#34;http://www.gdal.org/&#34;&gt;GDAL（Geospatial Data Abstraction Library）&lt;/a&gt;&lt;/strong&gt;。GDAL 是一个开源栅格空间数据转换库。有很多著名的 GIS 类产品都使用了GDAL/OGR库，包括 ESRI 的 ARCGIS 9.3，Google Earth 和跨平台的 GRASS GIS 系统。利用 GDAL/OGR 库，可以使基于 Linux 的地理空间数据管理系统提供对矢量和栅格文件数据的支持。&lt;/p&gt;

&lt;p&gt;Python 有调用 GDAL 的软件包，关于 GDAL 的安装和 python GDAL 软件包的安装可以参考其网站&lt;a href=&#34;https://pypi.python.org/pypi/GDAL/&#34;&gt;说明&lt;/a&gt;。
需要注意的是 Windows 和 linux 下 GDAL 的安装问题。&lt;/p&gt;

&lt;p&gt;GeoTiff 的读取方法 &lt;code&gt;read_geotiff&lt;/code&gt; 附后，默认参数表示读取 band 1 通道的数据（其实这里的数据只有 1 个 band），返回数据矩阵、地理变换参数、投影信息。&lt;/p&gt;

&lt;p&gt;ASTER GeoTiff 瓦片图的拼接方法 &lt;code&gt;combine_ASTER&lt;/code&gt; 亦附后，其中参数 &lt;code&gt;dirname&lt;/code&gt; 表示 geotiff 文件放置的位置，&lt;code&gt;ll&lt;/code&gt; 和 &lt;code&gt;ur&lt;/code&gt; 分别是左下（lower left）和右上（upper right）角的经纬度（&lt;strong&gt;注意&lt;/strong&gt;，这里指的是对应于 ASTER geotiff 文件的命名，而不是最后拼接出来的范围）。必要的注释附在代码里。&lt;/p&gt;

&lt;p&gt;以下是一个单幅图像（&lt;code&gt;ASTGTM2_N26E116_dem.tif&lt;/code&gt;）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ASTGTM2_N26E116_dem.png&#34; alt=&#34;ASTGTM2_N26E116_dem.tif&#34; title=&#34;ASTGTM2_N26E116_dem.tif&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以下是由 9 幅图像拼接后的图像（&lt;code&gt;ASTGTM2_N26E116_dem.tif&lt;/code&gt; 在左下角）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ASTGTM2_dem_merged.png&#34; alt=&#34;ASTGTM2_dem_merged.tif&#34; title=&#34;ASTGTM2_dem_merged.tif&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import gdal
import numpy as np
import matplotlib.pyplot as plt
from osgeo import gdal, gdalconst, osr
from gdalconst import *


def read_geotiff(tif_fn, band=1, isPlot=False, plot_resolution=1):
    &#39;&#39;&#39;
    Arguments:
        tif_fn {str} -- file name to be read
    
    Keyword Arguments:
        band {number} -- band (default: {1})
        isPlot {bool} -- view or not (default: {False})
        plot_resolution {number} -- increase this num when image is too large to display (default: {1})
    &#39;&#39;&#39;
    gdal.UseExceptions()
    ds = gdal.Open(tif_fn)
    band = ds.GetRasterBand(band)
    data = band.ReadAsArray()
    nrows, ncols = data.shape
    x0, dx, dxdy, y0, dydx, dy = ds.GetGeoTransform()  # [x0,y0] left bottom point
    x1 = x0 + dx * ncols
    y1 = y0 + dy * nrows  # [x1, y1] right top point
    geoTransform = ds.GetGeoTransform()
    projection = ds.GetProjection()
    # print(nrows, ncols)
    # print(geoTransform)
    # print(projection)
    if isPlot:
        plt.imshow(data[::plot_resolution, ::plot_resolution], cmap=&#39;gist_earth&#39;, extent=[x0, x1, y1, y0])
        cbar = plt.colorbar(orientation=&#39;vertical&#39;, shrink=0.5)
        plt.show()
    return data, geoTransform, projection


def combine_ASTER(dirname, ll=(116, 26), ur=(120, 32), isPreView=True, out_fn=&#39;merged.tif&#39;):

    def bbox_to_fns(ll=ll, ur=ur):
        y, x = np.mgrid[ll[1]:ur[1]+1:1, ll[0]:ur[0]+1:1]
        shp = x.shape
        x, y = [_.ravel() for _ in [x, y]]
        points = [[x[_], y[_]] for _ in range(len(x))]
        fn = &#39;ASTGTM2_N{}E{}_dem.tif&#39;
        fns = [fn.format(points[_][1], points[_][0]) for _ in range(len(points))]
        return fns, shp

    fns, shp = bbox_to_fns(ll=ll, ur=ur)
    # print fns

    def get_postion(fn):
        import re
        position = re.findall(r&#39;N(\d{1,5})E(\d{1,5})&#39;, fn)[0]  # only for N ande E, not S and W
        position = [float(_) for _ in position]
        return position

    # 画出文件命名中的左下角点位，用于查看要拼接的范围
    if isPreView:
        plt.hold(True)
        datas, lblons, rtlats, positions = [list(range(len(fns))) for i in range(4)]
        for _ in positions:
            fn_full = os.path.join(dirname, fns[_])
            positions[_] = get_postion(fn_full)
            plt.plot(positions[_][1], positions[_][0], &#39;ro&#39;)
            datas[_], geoTransform, projection = read_geotiff(fn_full)
            lblons[_], rtlats[_] = geoTransform[0], geoTransform[3]
        plt.show()

    # 拼接后图像的地理变换信息
    geoTransform = list(geoTransform)
    geoTransform[0], geoTransform[3] = lblons[0], rtlats[-1]  # lblon 取最小（不变），rtlat 取最大
    geoTransform = tuple(geoTransform)

    # 数据拼接（没有找到现成的方法，自己实现一下）
    col, row = shp
    cols = list(range(col))
    for _ in cols:
        cols[_] = np.hstack(np.array(datas)[_*row:(_+1)*row, :, :])
    datas = np.vstack(cols[::-1])
    height, width = datas.shape
    # print(height, width)

    # 写出 GeoTiff 数据文件（注意高程数据数值范围超过 255，不能用 gdal.GDT_Byte）
    driver = gdal.GetDriverByName(&#39;GTiff&#39;)
    ds = driver.Create(out_fn, width, height, 1, gdal.GDT_UInt16)  # band No. 1  # !!! gdal.GDT_UInt16
    ds.SetProjection(projection)
    ds.SetGeoTransform(geoTransform)
    ds.GetRasterBand(1).WriteArray(datas, 0, 0)
    ds = None

    return datas, geoTransform, projection


if __name__ == &#39;__main__&#39;:
    # 这里个人习惯上都是用于测试校验结果的

    data, geoTransform, projection = read_geotiff(
        os.path.join(&#39;ASTER_zhejiang&#39;, &#39;dem&#39;, &#39;ASTGTM2_N26E116_dem.tif&#39;), isPlot=True)
    print geoTransform
    print projection
    print data

    dirname = os.path.join(&#39;ASTER_zhejiang&#39;, &#39;dem&#39;)
    ll, ur = (116, 26), (118, 28)
    combine_ASTER(dirname, ll, ur, out_fn=&#39;merged.tif&#39;)

    datas, geoTransform, projection = read_geotiff(&#39;merged.tif&#39;, isPlot=True, plot_resolution=6)
    print geoTransform
    print datas

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>数据搜集和整理：requests和pandas的简单应用示例</title>
      <link>https://wormone.github.io/notebook/post/%E6%95%B0%E6%8D%AE%E6%90%9C%E9%9B%86%E5%92%8C%E6%95%B4%E7%90%86%EF%BC%9Arequests%E5%92%8Cpandas%E7%9A%84%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/</link>
      <pubDate>Tue, 31 Oct 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%95%B0%E6%8D%AE%E6%90%9C%E9%9B%86%E5%92%8C%E6%95%B4%E7%90%86%EF%BC%9Arequests%E5%92%8Cpandas%E7%9A%84%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/</guid>
      <description>&lt;p&gt;都说现在是大数据时代，不仅仅气象被数据记录下来，这个世界的方方面面都被或多或少的数据记录着。&lt;/p&gt;

&lt;p&gt;最大的问题就是数据资源太&lt;strong&gt;不平衡&lt;/strong&gt;了，有些地方数据大量冗余，价值有待更多的发掘，而有些地方却苦于数据资源太少，或者难以获得，很多有价值的工作无法开展……&lt;/p&gt;

&lt;p&gt;数据的搜集整理是一项很基础的工作，今天我们用pytnon的requests和pandas两个软件包试验一下获取淘宝每天的关注上升榜单Top100，地址是&lt;a href=&#34;https://top.taobao.com/index.php&#34;&gt;top.taobao.com&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;先通过浏览器分析数据加载的情况。开始一段时间从F12里没有找到数据来源，感觉有点奇怪，难道非要用selenium了吗？后来从分页的链接地址找到了数据接口，尝试curl之后发现还好，数据比较简单（但是浏览器打开的分页地址总会马上跳转到首页地址，网页源码也会变成首页源码，看不到新加载的数据）。&lt;/p&gt;

&lt;p&gt;所以我们直接采用requests就可以抓取了，然后再用正则匹配一下我们需要的数据，从字符串转换为json，再转换为pandas的DataFrame，方便转换为csv文件存储或者写入数据库。代码如下，python总是很简单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import re
import requests
import pandas as pd
import simplejson as json
from datetime import datetime, date, timedelta

&amp;quot;&amp;quot;&amp;quot;taobao daily top search 100

https://top.taobao.com/index.php?rank=focus&amp;amp;type=up
https://top.taobao.com/index.php?rank=focus&amp;amp;type=up&amp;amp;s=20
https://top.taobao.com/index.php?rank=focus&amp;amp;type=up&amp;amp;s=40
https://top.taobao.com/index.php?rank=focus&amp;amp;type=up&amp;amp;s=60
https://top.taobao.com/index.php?rank=focus&amp;amp;type=up&amp;amp;s=80
&amp;quot;&amp;quot;&amp;quot;


upOrDown = {
    1: 1,   # 1 means up
    2: 0,   # 2 means no change
    0: -1,  # 0 means down
}


def taobao_top100():
    url = &#39;https://top.taobao.com/index.php&#39;
    params = dict(rank=&#39;focus&#39;, type=&#39;up&#39;)
    data = []
    for i in range(7):  # 5
        params.update(dict(s=i*20))
        r = requests.get(url, params=params)
        if r.status_code == requests.codes.ok:
            text = r.text
            text = re.search(r&#39;(?&amp;lt;=&amp;quot;list&amp;quot;:)\[.*?\]&#39;, text).group(0)
            text = json.loads(text)
            # print text
            rank = [_[&#39;col1&#39;][&#39;text&#39;] for _ in text]
            name = [_[&#39;col2&#39;][&#39;text&#39;] for _ in text]
            search_num = [_[&#39;col4&#39;][&#39;num&#39;] for _ in text]
            rank_change = [_[&#39;col5&#39;][&#39;text&#39;] * upOrDown.get(_[&#39;col5&#39;][&#39;upOrDown&#39;]) for _ in text]
            rank_change_ratio = [(&#39;-&#39; if upOrDown.get(_[&#39;col6&#39;][&#39;upOrDown&#39;])&amp;lt;0 else &#39;&#39;) + _[&#39;col6&#39;][&#39;text&#39;] for _ in text]
            text = pd.DataFrame(dict(
                name=name, search_num=search_num, 
                rank_change=rank_change, rank_change_ratio=rank_change_ratio), index=rank)
            data.append(text)
        else:
            print(&#39;Error with code {}&#39;.format(r.status_code))
    data = pd.concat(data)
    data.to_csv(&#39;{}.csv&#39;.format(datetime.now().strftime(&#39;%Y%m%d%H%M%S&#39;)), encoding=&#39;utf8&#39;)
    return data


if __name__ == &#39;__main__&#39;:
    taobao_top100()

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>基于DBSCAN的探索性气候分析</title>
      <link>https://wormone.github.io/notebook/post/%E5%9F%BA%E4%BA%8EDBSCAN%E7%9A%84%E6%8E%A2%E7%B4%A2%E6%80%A7%E6%B0%94%E5%80%99%E5%88%86%E6%9E%90/</link>
      <pubDate>Wed, 27 Sep 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E5%9F%BA%E4%BA%8EDBSCAN%E7%9A%84%E6%8E%A2%E7%B4%A2%E6%80%A7%E6%B0%94%E5%80%99%E5%88%86%E6%9E%90/</guid>
      <description>&lt;p&gt;&lt;strong&gt;数据挖掘&lt;/strong&gt;是在大型数据存储库中，自动的发现有用信息的过程。数据挖掘技术用来探查大型数据库，发现先前未知的有用模式。&lt;/p&gt;

&lt;p&gt;数据挖掘的任务分为两大类：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;预测任务。目标是根据其它属性的值，预测特定属性的值。&lt;/li&gt;
&lt;li&gt;描述任务。目标是导出概括数据中潜在联系的模式（相关、趋势、聚类、轨迹和异常）。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;气象数据是天然的大数据，气象数据的分析研究其实都在遵循数据挖掘的规则，或者说是数据挖掘在这一特定领域的应用。&lt;/p&gt;

&lt;p&gt;大家知道，时空分布和变化是气象的基本属性和特征，对时空分布和变化的分析一直以来都是气象学研究关注的重点之一。常用的分析方法如：时间序列分析、高级谱分析、时空变量场的EOF、SVD、CCA分析等。不同的方法适用于不同的问题。&lt;/p&gt;

&lt;p&gt;关于气候分区的研究已经有很多了，但往往受限于数据和方法，我们对很多问题的认识仍然是有限的。&lt;/p&gt;

&lt;p&gt;今天这里的一个示例是基于DBSCAN聚类分析对降水量的气候特征进行一些探索性的分析。&lt;/p&gt;

&lt;p&gt;DBSCAN是一种简单、有效的基于&lt;strong&gt;密度&lt;/strong&gt;的聚类算法，&lt;strong&gt;寻找被低密度区域分离的高密度区域&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;我们对中国160个站1951-2016年夏季（JJA）降水量来做一些分析。距离的度量采用&lt;strong&gt;correlation&lt;/strong&gt;（注意不能用欧式距离）。两个参数Eps和MinPts，实验性的分别尝试[0.3, 0.4, 0.5, 0.6, 0.7]和[3, 4, 5, 6, 7, 8]。Eps是距离半径的阈值，MinPts是点个数的阈值。因此我们刚刚设定的参数搜索范围也就是相关性在0.7~0.3之间，簇内最少站点个数在3~8之间。&lt;/p&gt;

&lt;p&gt;由于DBSCAN使用簇的基于密度的定义，因此它是相对抗噪声的，并且能够处理任意形状和大小的簇，可以发现KMeans不能发现的很多簇。这是它的一个优点。&lt;/p&gt;

&lt;p&gt;以下是我们得到的一些有意思的结果（时间来不及了，回头再解释）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;DBSCAN_out_0.5_8.png&#34; alt=&#34;Eps=0.5, MinPts=8&#34; title=&#34;DBSCAN_out_0.5_8&#34; /&gt;
&lt;img src=&#34;DBSCAN_out_0.5_5.png&#34; alt=&#34;Eps=0.5, MinPts=5&#34; title=&#34;DBSCAN_out_0.5_5&#34; /&gt;
&lt;img src=&#34;DBSCAN_out_0.5_4.png&#34; alt=&#34;Eps=0.5, MinPts=4&#34; title=&#34;DBSCAN_out_0.5_4&#34; /&gt;
&lt;img src=&#34;DBSCAN_out_0.4_3.png&#34; alt=&#34;Eps=0.4, MinPts=3&#34; title=&#34;DBSCAN_out_0.4_3&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>看纪录片《地球起源HowTheEarthWasMade》</title>
      <link>https://wormone.github.io/notebook/post/%E7%9C%8B%E7%BA%AA%E5%BD%95%E7%89%87%E3%80%8A%E5%9C%B0%E7%90%83%E8%B5%B7%E6%BA%90HowTheEarthWasMade%E3%80%8B/</link>
      <pubDate>Wed, 13 Sep 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E7%9C%8B%E7%BA%AA%E5%BD%95%E7%89%87%E3%80%8A%E5%9C%B0%E7%90%83%E8%B5%B7%E6%BA%90HowTheEarthWasMade%E3%80%8B/</guid>
      <description>&lt;p&gt;这一集讲的是北美五大湖的形成。&lt;/p&gt;

&lt;p&gt;在我们普通人的眼里，我们游玩的山水风景都是不变的景观。其实任何一个地方的自然环境都是在不断演变着的，只是因为当我们用自己的的时间尺度来衡量时，这些变化显得&lt;strong&gt;太小&lt;/strong&gt;或者&lt;strong&gt;太慢&lt;/strong&gt;以至于被忽略了。&lt;/p&gt;

&lt;p&gt;然而，在地质学家眼中，一切都不同了。最早研究北美五大湖的地质学家测量发现，大瀑布每年以0.3米的速度侵蚀岩石，并由此推算出了五大湖的形成时间（再后来的研究发现侵蚀速度是每年0.9米）。在地质学家的眼里，如今的一切都是&lt;strong&gt;动态演变过程&lt;/strong&gt;中的一个微小&lt;strong&gt;片段&lt;/strong&gt;，地貌特征、地层结构、古生物化石、岩石类型、沉积物等等都无时无刻不伴随着这种演变，因而蕴藏着这种演变的秘密——它的&lt;strong&gt;过去和未来&lt;/strong&gt;！&lt;/p&gt;

&lt;p&gt;湖盆下的巨大盐矿、湖盆的白云岩、地表冰丘地貌、海洋生物化石、岩石上的冰川擦痕、古河系……当地质学家把越来越多的证据汇集起来，就能够推测出北美五大湖的形成过程，揭示出那些巨大的变迁。更有意思的是，如今令人担忧的五大湖湖水水位下降竟然是缘于“地表回弹”——过去被厚厚冰盖重重压低下沉的地表，当冰盖消失后正在回弹，海拔上升，流入湖中的河水减少甚至将会停止……&lt;/p&gt;

&lt;p&gt;其实，在地球自然环境变迁的背后，始终有一个无可匹敌的巨大推动力。它就是太阳。确切的说，不是太阳本身，而是地球接收的太阳辐射的变化。太阳活动、地球轨道参数、黄赤交角、大陆漂移、火山喷发的火山灰进入大气层形成的阳伞效应等等，这些都会影响地球上接收到的太阳辐射变化。&lt;strong&gt;辐射平衡&lt;/strong&gt;是地球气候系统形成的重要基础，辐射平衡的改变也就改变了气候，冷暖、干湿。终极一切奥秘，肯定都在天文宇宙之中。人类太渺小了，太短暂了，太微不足道了……&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习与环境气象：空气质量预报：4业务生产</title>
      <link>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A4%E4%B8%9A%E5%8A%A1%E7%94%9F%E4%BA%A7/</link>
      <pubDate>Fri, 21 Jul 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A4%E4%B8%9A%E5%8A%A1%E7%94%9F%E4%BA%A7/</guid>
      <description>&lt;p&gt;建立模型和方案的最终目的是能够应用于未来每天的实际预测业务生产中。因此，&lt;strong&gt;很重要的一点&lt;/strong&gt;必须记住，那就是未来可用于预测的特征必须能够和之前建模训练时采用的特征保持完全一致。也就是说，如果建模训练时采用的特征在后来不可获取了，那么模型也就无法应用到实际的日常生产中去了。所以，在建模训练的时候就必须关注这个问题。在条件不能再满足的情况下，重新建模训练甚至重新做特征工程都是有可能的。&lt;/p&gt;

&lt;p&gt;模型方案确定可行之后，要做的就是业务化部署了。这一部份工作就是对预报方案的封装，然后设定定时自动化任务并记录运行日志。如果是在linux系统上，crontab较为常用。如果实在windows上，也可以配置任务计划。此外，python也有软件包（常用apscheduler）可以做定时任务配置。&lt;/p&gt;

&lt;p&gt;既然是业务化生产，就要监控各种可能出现的异常情况，用python的logging模块可以很好的承担这个工作。对于各种异常情况造成的数据缺失情况，也应有相应的处理措施，具体问题需要具体分析，以保障日常生产。&lt;/p&gt;

&lt;p&gt;作为完整的业务流程，每天的预报结果通常以文件存储或写入数据库，这些方法一并封装在程序中就可以了。用python做这些工作通常都是非常简单的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习与环境气象：空气质量预报：3建模、训练和评估</title>
      <link>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A3%E5%BB%BA%E6%A8%A1%E3%80%81%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0/</link>
      <pubDate>Thu, 20 Jul 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A3%E5%BB%BA%E6%A8%A1%E3%80%81%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0/</guid>
      <description>&lt;p&gt;Python有很多机器学习方面的软件包（scikit-learn、TensorFlow、Keras等），对于我们应用于构建和训练模型都是非常方便的，官方文档就是很好的示例。工具都有了，剩下的问题就是具体问题具体分析，从理论角度&lt;strong&gt;选取合适的模型和方案&lt;/strong&gt;，然后尝试训练和评估模型的表现，最后获得可以用于日常生产的模型和方案。&lt;/p&gt;

&lt;p&gt;拿我们现在的空气质量预报问题来说，对于某一种污染物或者空气质量指数AQI，其伴随的天气特征是多元的，其关系可以是线性的也可以是非线性的。&lt;strong&gt;从适用的模型上来说&lt;/strong&gt;，人工神经网络模型、基于统计学的广义线性模型、基于决策树的集合方法，都可以尝试。&lt;strong&gt;从适用的方案上来说&lt;/strong&gt;，不同时空（不同地区、不同季节）匹配的条件下，空气质量与天气条件的关系也会不同，因此，对于不同地区、不同季节，应当分别进行建模、训练和评估。&lt;/p&gt;

&lt;p&gt;在实际运用于生产业务的时候，气象上还通常采用&lt;strong&gt;滚动预报&lt;/strong&gt;的方法，比如每天用之前一段时间的数据进行建模并用于未来若干天的预报。&lt;/p&gt;

&lt;p&gt;预报效果怎么样，模型和方案的评估就很重要。评估的对象取决于所关注的目标。一般，通过统计分析正确率、漏报率、空报率、均方根误差（RMSE）、平均绝对误差（MAE）等特征，可以做出较为全面的评估。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习与环境气象：空气质量预报：2数据处理</title>
      <link>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A2%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 19 Jul 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A2%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</guid>
      <description>&lt;p&gt;检查完数据质量问题，就要对发现的必要问题进行处理了。其中最重要的问题就是&lt;strong&gt;数据缺失或异常&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;对于发现的数据缺失或异常，如果其样本量不大，而且可以利用时间和空间关系进行&lt;strong&gt;插补&lt;/strong&gt;的话，那么还算比较好的情况，也是我们比较期望出现并能解决的情况。如果数据缺失或异常的样本量比较大，或者其它原因导致无法进行合理插补以弥补此类数据质量问题，那么我们很可能要被迫放弃这些数据，其后果是可能导致后续工作受到严重限制甚至无法进行，这是我们最头疼的清醒，但愿不要出现。&lt;/p&gt;

&lt;p&gt;在处理完数据质量问题之后，我们要做的工作就是对数据进行必要的转换，也就是进一步信息化，以便后续的模型构建和训练工作。因此，这一步要做的工作是非常重要的，如果没有做好&lt;strong&gt;必要的、合理的&lt;/strong&gt;数据转换工作，后续工作可能就无法取得比较理想的结果。&lt;/p&gt;

&lt;p&gt;以天气预报信息为例，风向（东南西北风）可以转换为数字标号或者360度，风力（微风、3-4级）可以转换为数字标号或者特定风速数值m/s，天气现象（晴、多云、阴、雨雪等等）则通常转换为0、1标签（0表示没出现，1表示有出现）。这些是原始信息的基本转换工作。&lt;/p&gt;

&lt;p&gt;还有一部分转换工作在于从专业角度分析影响结果的哪些&lt;strong&gt;特征&lt;/strong&gt;可以被提取出来。以我们现在的空气质量预报来说，天气条件不仅仅是当时的数值，&lt;strong&gt;天气的变化信息&lt;/strong&gt;反而更为重要，这就需要我们把他们从原始数据中萃取出来。比如，24小时的气温和气压变化、一天当中气温的日较差、风向的变化等等，它们对于空气质量来说，都可能是重要的影响的因素。换句话说，空气质量这样或者那样，伴随着的是这样或者那样的天气特征。因此，这一部分工作，就是机器学习中所说的&lt;strong&gt;特征工程&lt;/strong&gt;。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习与环境气象：空气质量预报：1爬取数据</title>
      <link>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A1%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Tue, 18 Jul 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A1%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE/</guid>
      <description>&lt;p&gt;从网上爬取数据是获取数据的重要途径，相比于文本类型的数据（比如用户评论），气象和环境方面的数据多以数字为主，爬取似乎简单一些。但是对于我们做研究来说，数据的完整性就比较重要，如果数据质量不好，会比较让人头疼。空气质量监测数据的数据源很多，官方的是环保总站公布的数据，还有很多第三方提供数据接口，可以仔细看看，选择&lt;strong&gt;数据质量好的、来源稳定的&lt;/strong&gt;。天气数据来源也很多，比如中国天气网官网，另外也有很多第三方数据源可以获取（他们的数据大多也来自气象部门，差别不大），也要选取&lt;strong&gt;数据质量好的、来源稳定的&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;对于我们的问题来说，有一点需要&lt;strong&gt;特别注意&lt;/strong&gt;，那就是无论气象数据还是空气质量数据，不光要有每天实时更新的数据（天气预报数据和空气质量实况数据），还需要有历史实况数据。还好这样的数据源都是可以找到的。&lt;/p&gt;

&lt;p&gt;数据爬取之前需要做的是仔细分析一下数据来源，这可能会大大减少爬取数据的工作量。比如，如果同一数据源，即有电脑浏览器页面又有手机浏览器页面，那么手机浏览器页面一般比较简单，容易分析。再比如，对于Ajax加载的数据，利用浏览器自带的工具仔细分析一下数据来源的请求，比调上来就调用Selenium要简单得多。&lt;/p&gt;

&lt;p&gt;Python的话，我们直接采用requests这个工具就可以很轻松完成数据爬取的工作了。&lt;/p&gt;

&lt;p&gt;爬下来的数据格式可能不是我们想要的，那么接下来要做的工作就是必要的格式转换。通常，json和csv都是很不错的格式。如果数据量比较大，还可以考虑用数据库来存储，对于简单一些的应用，sqlite就够了。对于数据处理，python的numpy和pandas通常都是很好用的工具。对于数据的预览（人工检视），用matplotlib画个曲线图出来看看是很直观的。另外，还可以从数据的基本统计信息来检查数据质量（特别是数据缺失和异常），比如最大最小值、平均值、样本数、方差等等，对于排查数据质量问题都有帮助。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>