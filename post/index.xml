<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on A notebook on data analysis in python</title>
    <link>https://wormone.github.io/notebook/post/index.xml</link>
    <description>Recent content in Posts on A notebook on data analysis in python</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 27 Feb 2018 22:35:15 +0800</lastBuildDate>
    <atom:link href="https://wormone.github.io/notebook/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>向量（直线）与栅格图像的交点（像元）计算</title>
      <link>https://wormone.github.io/notebook/post/%E5%90%91%E9%87%8F%EF%BC%88%E7%9B%B4%E7%BA%BF%EF%BC%89%E4%B8%8E%E6%A0%85%E6%A0%BC%E5%9B%BE%E5%83%8F%E7%9A%84%E4%BA%A4%E7%82%B9%EF%BC%88%E5%83%8F%E5%85%83%EF%BC%89%E8%AE%A1%E7%AE%97/</link>
      <pubDate>Tue, 27 Feb 2018 22:35:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E5%90%91%E9%87%8F%EF%BC%88%E7%9B%B4%E7%BA%BF%EF%BC%89%E4%B8%8E%E6%A0%85%E6%A0%BC%E5%9B%BE%E5%83%8F%E7%9A%84%E4%BA%A4%E7%82%B9%EF%BC%88%E5%83%8F%E5%85%83%EF%BC%89%E8%AE%A1%E7%AE%97/</guid>
      <description>&lt;p&gt;这次还是一个卫星项目，问题是这样的：&lt;/p&gt;

&lt;p&gt;在一个栅格图像上，有一个点的位置上有一个向量，需要计算的是这个向量所在的直线所经过的像元有哪些。&lt;/p&gt;

&lt;p&gt;问题看上去简单，但似乎实现起来还有点麻烦。&lt;/p&gt;

&lt;p&gt;还好我们有 PIL 可以用，问题就简化成只要找到这条直线的两个&lt;strong&gt;端点&lt;/strong&gt;就行了，思路是这样的：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;这两个端点必然在栅格图像最外圈上，只要找到这上面的两个点就行了；&lt;/li&gt;
&lt;li&gt;首先提取最外圈所有像素的坐标，然后计算每个点与给定点之间的向量，计算该向量与所给向量之前&lt;strong&gt;相互平行&lt;/strong&gt;的一个度量；&lt;/li&gt;
&lt;li&gt;找到最为相互平行的两个度量所对应的点，那就基本大功告成了。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;最后利用 PIL，把找到的两个点在栅格图像上画出直线，找到直线经过的像素点就完成了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw


def vector_across_image(p=(10, 30), u=-1, v=2, size=100, isPlot=False):
    &amp;quot;&amp;quot;&amp;quot;vector line across a square image
    
    A point on the image with a direction like u,v components, make a vector,
    which across the image, result as a line of pixels
    
    Keyword Arguments:
        p {tuple} -- position of the point on the image (default: {(10, 30)})
        u {number} -- u component (default: {-1})
        v {number} -- v component (default: {2})
        size {number} -- image size (default: {100})
        isPlot {bool} -- view the result or not (default: {False})
    
    Returns:
        points -- pixels on the line
    &amp;quot;&amp;quot;&amp;quot;
    # make a square
    x, y = np.mgrid[:size, :size]
    # take the outline of the square
    outline = zip(x[0, :], y[0, :]) + zip(x[-1, :], y[-1, :]) \
            + zip(x[:, 0], y[:, 0]) + zip(x[:, -1], y[:, -1])
    outline = np.array(outline)

    # find the two interception points on the outline, i.e. p0 and p1
    # delta = [(_[0]-p[0])*v - (_[1]-p[1])*u for _ in outline]
    delta = (outline[:, 0] - p[0]) * v - (outline[:, 1] -p[1]) * u
    delta = np.abs(delta)
    ascding = np.argsort(delta)
    p0 = outline[ascding[0]]
    p1 = outline[ascding[1]]

    # get the line using PIL
    image = Image.new(&#39;1&#39;, (size, size))
    draw = ImageDraw.Draw(image)
    draw.line((p0[0], p0[1], p1[0], p1[1]), fill=&#39;white&#39;)
    color = list(image.getdata())
    color = np.reshape(color, (size, size))
    px = y[color==255].ravel()  # !!! x--&amp;gt;y
    py = x[color==255].ravel()  # !!! y--&amp;gt;x
    points = zip(px, py)

    if isPlot:
        plt.subplot(211)
        plt.plot(px, py, &#39;rs&#39;)
        plt.plot(p[0], p[1], &#39;ko&#39;, ms=20)
        plt.plot([p0[0], p1[0]], [p0[1], p1[1]], &#39;b-&#39;)
        plt.axis(&#39;scaled&#39;)
        plt.axis([0, size, 0, size])
        plt.subplot(212)
        plt.imshow(color)
        plt.show()

    return points, color


def test():
    points, color = vector_across_image(p=(10, 30), u=-1, v=2, size=100, isPlot=True)
    # print color


if __name__ == &#39;__main__&#39;:
    test()
    # from timeit import timeit
    # t = timeit(&#39;vector_across_image()&#39;, &#39;from __main__ import vector_across_image&#39;, number=140*140)
    # print(t)

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>中圆点算法（Midpoint circle algorithm）的简单实现</title>
      <link>https://wormone.github.io/notebook/post/%E4%B8%AD%E5%9C%86%E7%82%B9%E7%AE%97%E6%B3%95%EF%BC%88Midpoint%20circle%20algorithm%EF%BC%89%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/</link>
      <pubDate>Mon, 22 Jan 2018 14:35:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E4%B8%AD%E5%9C%86%E7%82%B9%E7%AE%97%E6%B3%95%EF%BC%88Midpoint%20circle%20algorithm%EF%BC%89%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/</guid>
      <description>&lt;p&gt;最近再做一个卫星遥感监测台风的项目，其中有个算法要提取台风中心一定半径的一圈像素然后做一些统计。查了查，原来这个算法还有个名字叫做&lt;a href=&#34;https://en.wikipedia.org/wiki/Midpoint_circle_algorithm&#34;&gt;&lt;strong&gt;中圆点算法（Midpoint circle algorithm）&lt;/strong&gt;&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;中圆点算法，说简单点就是设定一个半径画个圆，然后看看哪些像素在这个圆圈上。&lt;/p&gt;

&lt;p&gt;其实利用 PIL 可以很简单的实现中圆点算法的功能，找出哪些像素组成了圆圈。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;midpoint_circle_PIL.png&#34; alt=&#34;midpoint_circle_PIL.tif&#34; title=&#34;midpoint_circle_PIL.tif&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw


def get_midpoint_circle_pixels(size=100, radius=30, isPlot=False):
    &#39;&#39;&#39;get midpoint circle pixels easily with PIL
    
    Keyword Arguments:
        size {number} -- image size (default: {100})
        radius {number} -- circle radius, no more than size/2 (default: {30})
        isPlot {bool} -- show image or not (default: {False})
    
    Returns:
        x -- x coordinate
        y -- y coordinate
        points -- [[x[i], y[i]] for i in range(len(x))]
    &#39;&#39;&#39;

    # create new image, size x size pixels, 1 bit per pixel
    image = Image.new(&#39;1&#39;, (size, size))
    draw = ImageDraw.Draw(image)

    # actually a circle was drawn
    lower_left = size/2 - radius
    upper_right = size/2 + radius
    draw.ellipse((lower_left, lower_left, upper_right, upper_right), outline=&#39;white&#39;)

    # image.show()

    # get pixel values
    color = list(image.getdata())
    color = np.reshape(color, (size, size))

    # get pixel coordinate with color=&#39;white&#39;
    grid_x, grid_y = np.mgrid[:size, :size]
    x = grid_x[color==255].ravel()
    y = grid_y[color==255].ravel()
    points = [[x[_], y[_]] for _ in range(len(x))]

    if isPlot:
        plt.pcolor(color)
        plt.axis(&#39;scaled&#39;)
        plt.hold(True)
        plt.plot(x, y, &#39;wo&#39;)
        plt.show()

    return x, y, points


if __name__ == &#39;__main__&#39;:
    size = 100
    x1, y1, _ = get_midpoint_circle_pixels(size=size, radius=10)
    x2, y2, _ = get_midpoint_circle_pixels(size=size, radius=25)
    x3, y3, _ = get_midpoint_circle_pixels(size=size, radius=40)
    plt.hold(True)
    plt.plot(x1, y1, &#39;ro&#39;)
    plt.plot(x2, y2, &#39;go&#39;)
    plt.plot(x3, y3, &#39;bo&#39;)
    plt.axis(&#39;scaled&#39;)
    plt.xlim(0, size)
    plt.ylim(0, size)
    plt.show()

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;调用上面的方法，就得到的一组坐标点，但是这些点并没有按照画圆的方向排列……&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x, y, points = get_midpoint_circle_pixels(size=10, radius=3, isPlot=True)
print x
print y
print points
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[2 2 2 2 2 3 3 3 3 4 4 5 5 6 6 7 7 7 7 8 8 8 8 8]
[3 4 5 6 7 2 3 7 8 2 8 2 8 2 8 2 3 7 8 3 4 5 6 7]
[[2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [3, 2], [3, 3], [3, 7], [3, 8], [4, 2], [4, 8], [5, 2], [5, 8], [6, 2], [6, 8], [7, 2], [7, 3], [7, 7], [7, 8], [8, 3], [8, 4], [8, 5], [8, 6], [8, 7]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;midpoint_circle_points.png&#34; alt=&#34;midpoint_circle_points.tif&#34; title=&#34;midpoint_circle_points.tif&#34; /&gt;&lt;/p&gt;

&lt;p&gt;不过排列起来也并不困难，做法是这样的：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;先预定输出排列好的列表为 ring；&lt;/li&gt;
&lt;li&gt;从 points 里找到 &lt;strong&gt;center top&lt;/strong&gt; 的那一个点，放入 ring；&lt;/li&gt;
&lt;li&gt;然后按照顺时针方向找到下一个点追加到 ring 里面，直到结束；

&lt;ul&gt;
&lt;li&gt;这个点必然在上一个点周围的 8 个点之中；&lt;/li&gt;
&lt;li&gt;按照先&lt;strong&gt;上/右/下/左&lt;/strong&gt;然后&lt;strong&gt;右上/右下/左下/左上&lt;/strong&gt;顺序搜索；&lt;/li&gt;
&lt;li&gt;只要从这 8 个点里面找到一个点，当它在 points 里但是尚未在 ring 里，那就找到了；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;特别注意&lt;/strong&gt;，第一次搜索时，只需要找到唯一一个 next 点，这才能确保 ring 的排序方向。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;# sort points clockwise
ymax = max(y)
xmid = int(np.median(x))
first_point = [xmid, ymax]
ring = [first_point,]
for _ in range(len(points)):
    x, y = ring[-1]
    # search in the 8 points around: firstly top/right/bottom/left and secondly ur/lr/ll/ul
    possible = [
        [x, y+1], [x+1, y], [x, y-1], [x-1, y],
        [x+1, y+1], [x+1, y-1], [x-1, y-1], [x-1, y+1]]
    for p in possible:
        if (p in points) and (p not in ring):
            ring.append(p)
            # at the first point, find the only one next point: this will insure the direction
            if len(ring) == 2:
                break
x = [ring[_][0] for _ in range(len(ring))]
y = [ring[_][1] for _ in range(len(ring))]
points = [[x[_], y[_]] for _ in range(len(x))]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这样就完成了 x, y, points 的排列。这几句话可以追加到 &lt;code&gt;get_midpoint_circle_pixels()&lt;/code&gt; 方法里，直接输出排序结果。最后画个图出来验证一下排序结果是否正确：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;size = 50
for r in range(1, 23):
    x, y, points = get_midpoint_circle_pixels(size=size, radius=r)
    plt.plot(x, y, &#39;-s&#39;, label=str(r))
    plt.axis(&#39;scaled&#39;)
    plt.xlim(0, size)
    plt.ylim(0, size)
    # plt.legend(loc=&#39;best&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;midpoint_circle_ring.png&#34; alt=&#34;midpoint_circle_ring.tif&#34; title=&#34;midpoint_circle_ring.tif&#34; /&gt;&lt;/p&gt;

&lt;p&gt;结果是正确的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GeoTiff图像拼接</title>
      <link>https://wormone.github.io/notebook/post/GeoTiff%E5%9B%BE%E5%83%8F%E6%8B%BC%E6%8E%A5/</link>
      <pubDate>Fri, 29 Dec 2017 09:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/GeoTiff%E5%9B%BE%E5%83%8F%E6%8B%BC%E6%8E%A5/</guid>
      <description>&lt;p&gt;&lt;strong&gt;GeoTiff&lt;/strong&gt; 作为 TIFF 的一种扩展，在 TIFF 的基础上定义了一些地理标签， 来对各种坐标系统、椭球基准、投影信息等进行定义和存储，使图像数据和地理数据存储在同一图像文件中。&lt;/p&gt;

&lt;p&gt;今天我们要用到的数据是 ASTER 数字高程地形数据，可以在 &lt;a href=&#34;https://earthexplorer.usgs.gov/&#34;&gt;USGS Earth Explorer site&lt;/a&gt; 注册后，自定义区域下载。&lt;/p&gt;

&lt;p&gt;ASTER 每幅图像是范围为 1° × 1° 且分辨率为 1 arc-second (~30m) 的 GeoTiff 格式文件。其&lt;strong&gt;命名方式&lt;/strong&gt;类似于：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;ASTGTM2_N26E116_dem.tif&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其中 &lt;code&gt;N26E116&lt;/code&gt; 表示左下角的经纬度坐标。这幅图像的&lt;strong&gt;地理变换信息&lt;/strong&gt;信息如下：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;(115.99986111111112, 0.0002777777777777778, 0.0, 27.000138888888888, 0.0, -0.0002777777777777778)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;其对应的分别是：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x0, dx, dxdy, y0, dydx, dy&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;x0 即最小经度，dx 和 dy 是经纬度方向的变化率（数值等于经纬度范围除以格点数），&lt;strong&gt;注意&lt;/strong&gt;这里 dy 为负值，因此 y0 为最大纬度。如果 dy 为正，则 y0 应为最小纬度。&lt;/p&gt;

&lt;p&gt;这幅图像的&lt;strong&gt;投影信息&lt;/strong&gt;如下（其实 ASTER 同一数据源的所有图像的投影信息都是一样的）：&lt;/p&gt;

&lt;p&gt;&lt;code&gt;GEOGCS[&amp;quot;WGS 84&amp;quot;,DATUM[&amp;quot;WGS_1984&amp;quot;,SPHEROID[&amp;quot;WGS 84&amp;quot;,6378137,298.257223563,AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;7030&amp;quot;]],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;6326&amp;quot;]],PRIMEM[&amp;quot;Greenwich&amp;quot;,0],UNIT[&amp;quot;degree&amp;quot;,0.0174532925199433],AUTHORITY[&amp;quot;EPSG&amp;quot;,&amp;quot;4326&amp;quot;]]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;我们今天要做的是将若干幅数字图像拼接起来，形成一个更大的图像。简单来说就是像拼积木一样拼起来，道理很简单。&lt;/p&gt;

&lt;p&gt;我们利用的工具是 &lt;strong&gt;&lt;a href=&#34;http://www.gdal.org/&#34;&gt;GDAL（Geospatial Data Abstraction Library）&lt;/a&gt;&lt;/strong&gt;。GDAL 是一个开源栅格空间数据转换库。有很多著名的 GIS 类产品都使用了GDAL/OGR库，包括 ESRI 的 ARCGIS 9.3，Google Earth 和跨平台的 GRASS GIS 系统。利用 GDAL/OGR 库，可以使基于 Linux 的地理空间数据管理系统提供对矢量和栅格文件数据的支持。&lt;/p&gt;

&lt;p&gt;Python 有调用 GDAL 的软件包，关于 GDAL 的安装和 python GDAL 软件包的安装可以参考其网站&lt;a href=&#34;https://pypi.python.org/pypi/GDAL/&#34;&gt;说明&lt;/a&gt;。
需要注意的是 Windows 和 linux 下 GDAL 的安装问题。&lt;/p&gt;

&lt;p&gt;GeoTiff 的读取方法 &lt;code&gt;read_geotiff&lt;/code&gt; 附后，默认参数表示读取 band 1 通道的数据（其实这里的数据只有 1 个 band），返回数据矩阵、地理变换参数、投影信息。&lt;/p&gt;

&lt;p&gt;ASTER GeoTiff 瓦片图的拼接方法 &lt;code&gt;combine_ASTER&lt;/code&gt; 亦附后，其中参数 &lt;code&gt;dirname&lt;/code&gt; 表示 geotiff 文件放置的位置，&lt;code&gt;ll&lt;/code&gt; 和 &lt;code&gt;ur&lt;/code&gt; 分别是左下（lower left）和右上（upper right）角的经纬度（&lt;strong&gt;注意&lt;/strong&gt;，这里指的是对应于 ASTER geotiff 文件的命名，而不是最后拼接出来的范围）。必要的注释附在代码里。&lt;/p&gt;

&lt;p&gt;以下是一个单幅图像（&lt;code&gt;ASTGTM2_N26E116_dem.tif&lt;/code&gt;）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ASTGTM2_N26E116_dem.png&#34; alt=&#34;ASTGTM2_N26E116_dem.tif&#34; title=&#34;ASTGTM2_N26E116_dem.tif&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以下是由 9 幅图像拼接后的图像（&lt;code&gt;ASTGTM2_N26E116_dem.tif&lt;/code&gt; 在左下角）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;ASTGTM2_dem_merged.png&#34; alt=&#34;ASTGTM2_dem_merged.tif&#34; title=&#34;ASTGTM2_dem_merged.tif&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import gdal
import numpy as np
import matplotlib.pyplot as plt
from osgeo import gdal, gdalconst, osr
from gdalconst import *


def read_geotiff(tif_fn, band=1, isPlot=False, plot_resolution=1):
    &#39;&#39;&#39;
    Arguments:
        tif_fn {str} -- file name to be read
    
    Keyword Arguments:
        band {number} -- band (default: {1})
        isPlot {bool} -- view or not (default: {False})
        plot_resolution {number} -- increase this num when image is too large to display (default: {1})
    &#39;&#39;&#39;
    gdal.UseExceptions()
    ds = gdal.Open(tif_fn)
    band = ds.GetRasterBand(band)
    data = band.ReadAsArray()
    nrows, ncols = data.shape
    x0, dx, dxdy, y0, dydx, dy = ds.GetGeoTransform()  # [x0,y0] left bottom point
    x1 = x0 + dx * ncols
    y1 = y0 + dy * nrows  # [x1, y1] right top point
    geoTransform = ds.GetGeoTransform()
    projection = ds.GetProjection()
    # print(nrows, ncols)
    # print(geoTransform)
    # print(projection)
    if isPlot:
        plt.imshow(data[::plot_resolution, ::plot_resolution], cmap=&#39;gist_earth&#39;, extent=[x0, x1, y1, y0])
        cbar = plt.colorbar(orientation=&#39;vertical&#39;, shrink=0.5)
        plt.show()
    return data, geoTransform, projection


def combine_ASTER(dirname, ll=(116, 26), ur=(120, 32), isPreView=True, out_fn=&#39;merged.tif&#39;):

    def bbox_to_fns(ll=ll, ur=ur):
        y, x = np.mgrid[ll[1]:ur[1]+1:1, ll[0]:ur[0]+1:1]
        shp = x.shape
        x, y = [_.ravel() for _ in [x, y]]
        points = [[x[_], y[_]] for _ in range(len(x))]
        fn = &#39;ASTGTM2_N{}E{}_dem.tif&#39;
        fns = [fn.format(points[_][1], points[_][0]) for _ in range(len(points))]
        return fns, shp

    fns, shp = bbox_to_fns(ll=ll, ur=ur)
    # print fns

    def get_postion(fn):
        import re
        position = re.findall(r&#39;N(\d{1,5})E(\d{1,5})&#39;, fn)[0]  # only for N ande E, not S and W
        position = [float(_) for _ in position]
        return position

    # 画出文件命名中的左下角点位，用于查看要拼接的范围
    if isPreView:
        plt.hold(True)
        datas, lblons, rtlats, positions = [list(range(len(fns))) for i in range(4)]
        for _ in positions:
            fn_full = os.path.join(dirname, fns[_])
            positions[_] = get_postion(fn_full)
            plt.plot(positions[_][1], positions[_][0], &#39;ro&#39;)
            datas[_], geoTransform, projection = read_geotiff(fn_full)
            lblons[_], rtlats[_] = geoTransform[0], geoTransform[3]
        plt.show()

    # 拼接后图像的地理变换信息
    geoTransform = list(geoTransform)
    geoTransform[0], geoTransform[3] = lblons[0], rtlats[-1]  # lblon 取最小（不变），rtlat 取最大
    geoTransform = tuple(geoTransform)

    # 数据拼接（没有找到现成的方法，自己实现一下）
    col, row = shp
    cols = list(range(col))
    for _ in cols:
        cols[_] = np.hstack(np.array(datas)[_*row:(_+1)*row, :, :])
    datas = np.vstack(cols[::-1])
    height, width = datas.shape
    # print(height, width)

    # 写出 GeoTiff 数据文件（注意高程数据数值范围超过 255，不能用 gdal.GDT_Byte）
    driver = gdal.GetDriverByName(&#39;GTiff&#39;)
    ds = driver.Create(out_fn, width, height, 1, gdal.GDT_UInt16)  # band No. 1  # !!! gdal.GDT_UInt16
    ds.SetProjection(projection)
    ds.SetGeoTransform(geoTransform)
    ds.GetRasterBand(1).WriteArray(datas, 0, 0)
    ds = None

    return datas, geoTransform, projection


if __name__ == &#39;__main__&#39;:
    # 这里个人习惯上都是用于测试校验结果的

    data, geoTransform, projection = read_geotiff(
        os.path.join(&#39;ASTER_zhejiang&#39;, &#39;dem&#39;, &#39;ASTGTM2_N26E116_dem.tif&#39;), isPlot=True)
    print geoTransform
    print projection
    print data

    dirname = os.path.join(&#39;ASTER_zhejiang&#39;, &#39;dem&#39;)
    ll, ur = (116, 26), (118, 28)
    combine_ASTER(dirname, ll, ur, out_fn=&#39;merged.tif&#39;)

    datas, geoTransform, projection = read_geotiff(&#39;merged.tif&#39;, isPlot=True, plot_resolution=6)
    print geoTransform
    print datas

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>数据搜集和整理：requests和pandas的简单应用示例</title>
      <link>https://wormone.github.io/notebook/post/%E6%95%B0%E6%8D%AE%E6%90%9C%E9%9B%86%E5%92%8C%E6%95%B4%E7%90%86%EF%BC%9Arequests%E5%92%8Cpandas%E7%9A%84%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/</link>
      <pubDate>Tue, 31 Oct 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%95%B0%E6%8D%AE%E6%90%9C%E9%9B%86%E5%92%8C%E6%95%B4%E7%90%86%EF%BC%9Arequests%E5%92%8Cpandas%E7%9A%84%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/</guid>
      <description>&lt;p&gt;都说现在是大数据时代，不仅仅气象被数据记录下来，这个世界的方方面面都被或多或少的数据记录着。&lt;/p&gt;

&lt;p&gt;最大的问题就是数据资源太&lt;strong&gt;不平衡&lt;/strong&gt;了，有些地方数据大量冗余，价值有待更多的发掘，而有些地方却苦于数据资源太少，或者难以获得，很多有价值的工作无法开展……&lt;/p&gt;

&lt;p&gt;数据的搜集整理是一项很基础的工作，今天我们用pytnon的requests和pandas两个软件包试验一下获取淘宝每天的关注上升榜单Top100，地址是&lt;a href=&#34;https://top.taobao.com/index.php&#34;&gt;top.taobao.com&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;先通过浏览器分析数据加载的情况。开始一段时间从F12里没有找到数据来源，感觉有点奇怪，难道非要用selenium了吗？后来从分页的链接地址找到了数据接口，尝试curl之后发现还好，数据比较简单（但是浏览器打开的分页地址总会马上跳转到首页地址，网页源码也会变成首页源码，看不到新加载的数据）。&lt;/p&gt;

&lt;p&gt;所以我们直接采用requests就可以抓取了，然后再用正则匹配一下我们需要的数据，从字符串转换为json，再转换为pandas的DataFrame，方便转换为csv文件存储或者写入数据库。代码如下，python总是很简单：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

import re
import requests
import pandas as pd
import simplejson as json
from datetime import datetime, date, timedelta

&amp;quot;&amp;quot;&amp;quot;taobao daily top search 100

https://top.taobao.com/index.php?rank=focus&amp;amp;type=up
https://top.taobao.com/index.php?rank=focus&amp;amp;type=up&amp;amp;s=20
https://top.taobao.com/index.php?rank=focus&amp;amp;type=up&amp;amp;s=40
https://top.taobao.com/index.php?rank=focus&amp;amp;type=up&amp;amp;s=60
https://top.taobao.com/index.php?rank=focus&amp;amp;type=up&amp;amp;s=80
&amp;quot;&amp;quot;&amp;quot;


upOrDown = {
    1: 1,   # 1 means up
    2: 0,   # 2 means no change
    0: -1,  # 0 means down
}


def taobao_top100():
    url = &#39;https://top.taobao.com/index.php&#39;
    params = dict(rank=&#39;focus&#39;, type=&#39;up&#39;)
    data = []
    for i in range(7):  # 5
        params.update(dict(s=i*20))
        r = requests.get(url, params=params)
        if r.status_code == requests.codes.ok:
            text = r.text
            text = re.search(r&#39;(?&amp;lt;=&amp;quot;list&amp;quot;:)\[.*?\]&#39;, text).group(0)
            text = json.loads(text)
            # print text
            rank = [_[&#39;col1&#39;][&#39;text&#39;] for _ in text]
            name = [_[&#39;col2&#39;][&#39;text&#39;] for _ in text]
            search_num = [_[&#39;col4&#39;][&#39;num&#39;] for _ in text]
            rank_change = [_[&#39;col5&#39;][&#39;text&#39;] * upOrDown.get(_[&#39;col5&#39;][&#39;upOrDown&#39;]) for _ in text]
            rank_change_ratio = [(&#39;-&#39; if upOrDown.get(_[&#39;col6&#39;][&#39;upOrDown&#39;])&amp;lt;0 else &#39;&#39;) + _[&#39;col6&#39;][&#39;text&#39;] for _ in text]
            text = pd.DataFrame(dict(
                name=name, search_num=search_num, 
                rank_change=rank_change, rank_change_ratio=rank_change_ratio), index=rank)
            data.append(text)
        else:
            print(&#39;Error with code {}&#39;.format(r.status_code))
    data = pd.concat(data)
    data.to_csv(&#39;{}.csv&#39;.format(datetime.now().strftime(&#39;%Y%m%d%H%M%S&#39;)), encoding=&#39;utf8&#39;)
    return data


if __name__ == &#39;__main__&#39;:
    taobao_top100()

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>基于DBSCAN的探索性气候分析</title>
      <link>https://wormone.github.io/notebook/post/%E5%9F%BA%E4%BA%8EDBSCAN%E7%9A%84%E6%8E%A2%E7%B4%A2%E6%80%A7%E6%B0%94%E5%80%99%E5%88%86%E6%9E%90/</link>
      <pubDate>Wed, 27 Sep 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E5%9F%BA%E4%BA%8EDBSCAN%E7%9A%84%E6%8E%A2%E7%B4%A2%E6%80%A7%E6%B0%94%E5%80%99%E5%88%86%E6%9E%90/</guid>
      <description>&lt;p&gt;&lt;strong&gt;数据挖掘&lt;/strong&gt;是在大型数据存储库中，自动的发现有用信息的过程。数据挖掘技术用来探查大型数据库，发现先前未知的有用模式。&lt;/p&gt;

&lt;p&gt;数据挖掘的任务分为两大类：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;预测任务。目标是根据其它属性的值，预测特定属性的值。&lt;/li&gt;
&lt;li&gt;描述任务。目标是导出概括数据中潜在联系的模式（相关、趋势、聚类、轨迹和异常）。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;气象数据是天然的大数据，气象数据的分析研究其实都在遵循数据挖掘的规则，或者说是数据挖掘在这一特定领域的应用。&lt;/p&gt;

&lt;p&gt;大家知道，时空分布和变化是气象的基本属性和特征，对时空分布和变化的分析一直以来都是气象学研究关注的重点之一。常用的分析方法如：时间序列分析、高级谱分析、时空变量场的EOF、SVD、CCA分析等。不同的方法适用于不同的问题。&lt;/p&gt;

&lt;p&gt;关于气候分区的研究已经有很多了，但往往受限于数据和方法，我们对很多问题的认识仍然是有限的。&lt;/p&gt;

&lt;p&gt;今天这里的一个示例是基于DBSCAN聚类分析对降水量的气候特征进行一些探索性的分析。&lt;/p&gt;

&lt;p&gt;DBSCAN是一种简单、有效的基于&lt;strong&gt;密度&lt;/strong&gt;的聚类算法，&lt;strong&gt;寻找被低密度区域分离的高密度区域&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;我们对中国160个站1951-2016年夏季（JJA）降水量来做一些分析。距离的度量采用&lt;strong&gt;correlation&lt;/strong&gt;（注意不能用欧式距离）。两个参数Eps和MinPts，实验性的分别尝试[0.3, 0.4, 0.5, 0.6, 0.7]和[3, 4, 5, 6, 7, 8]。Eps是距离半径的阈值，MinPts是点个数的阈值。因此我们刚刚设定的参数搜索范围也就是相关性在0.7~0.3之间，簇内最少站点个数在3~8之间。&lt;/p&gt;

&lt;p&gt;由于DBSCAN使用簇的基于密度的定义，因此它是相对抗噪声的，并且能够处理任意形状和大小的簇，可以发现KMeans不能发现的很多簇。这是它的一个优点。&lt;/p&gt;

&lt;p&gt;以下是我们得到的一些有意思的结果（时间来不及了，回头再解释）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;DBSCAN_out_0.5_8.png&#34; alt=&#34;Eps=0.5, MinPts=8&#34; title=&#34;DBSCAN_out_0.5_8&#34; /&gt;
&lt;img src=&#34;DBSCAN_out_0.5_5.png&#34; alt=&#34;Eps=0.5, MinPts=5&#34; title=&#34;DBSCAN_out_0.5_5&#34; /&gt;
&lt;img src=&#34;DBSCAN_out_0.5_4.png&#34; alt=&#34;Eps=0.5, MinPts=4&#34; title=&#34;DBSCAN_out_0.5_4&#34; /&gt;
&lt;img src=&#34;DBSCAN_out_0.4_3.png&#34; alt=&#34;Eps=0.4, MinPts=3&#34; title=&#34;DBSCAN_out_0.4_3&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>看纪录片《地球起源HowTheEarthWasMade》</title>
      <link>https://wormone.github.io/notebook/post/%E7%9C%8B%E7%BA%AA%E5%BD%95%E7%89%87%E3%80%8A%E5%9C%B0%E7%90%83%E8%B5%B7%E6%BA%90HowTheEarthWasMade%E3%80%8B/</link>
      <pubDate>Wed, 13 Sep 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E7%9C%8B%E7%BA%AA%E5%BD%95%E7%89%87%E3%80%8A%E5%9C%B0%E7%90%83%E8%B5%B7%E6%BA%90HowTheEarthWasMade%E3%80%8B/</guid>
      <description>&lt;p&gt;这一集讲的是北美五大湖的形成。&lt;/p&gt;

&lt;p&gt;在我们普通人的眼里，我们游玩的山水风景都是不变的景观。其实任何一个地方的自然环境都是在不断演变着的，只是因为当我们用自己的的时间尺度来衡量时，这些变化显得&lt;strong&gt;太小&lt;/strong&gt;或者&lt;strong&gt;太慢&lt;/strong&gt;以至于被忽略了。&lt;/p&gt;

&lt;p&gt;然而，在地质学家眼中，一切都不同了。最早研究北美五大湖的地质学家测量发现，大瀑布每年以0.3米的速度侵蚀岩石，并由此推算出了五大湖的形成时间（再后来的研究发现侵蚀速度是每年0.9米）。在地质学家的眼里，如今的一切都是&lt;strong&gt;动态演变过程&lt;/strong&gt;中的一个微小&lt;strong&gt;片段&lt;/strong&gt;，地貌特征、地层结构、古生物化石、岩石类型、沉积物等等都无时无刻不伴随着这种演变，因而蕴藏着这种演变的秘密——它的&lt;strong&gt;过去和未来&lt;/strong&gt;！&lt;/p&gt;

&lt;p&gt;湖盆下的巨大盐矿、湖盆的白云岩、地表冰丘地貌、海洋生物化石、岩石上的冰川擦痕、古河系……当地质学家把越来越多的证据汇集起来，就能够推测出北美五大湖的形成过程，揭示出那些巨大的变迁。更有意思的是，如今令人担忧的五大湖湖水水位下降竟然是缘于“地表回弹”——过去被厚厚冰盖重重压低下沉的地表，当冰盖消失后正在回弹，海拔上升，流入湖中的河水减少甚至将会停止……&lt;/p&gt;

&lt;p&gt;其实，在地球自然环境变迁的背后，始终有一个无可匹敌的巨大推动力。它就是太阳。确切的说，不是太阳本身，而是地球接收的太阳辐射的变化。太阳活动、地球轨道参数、黄赤交角、大陆漂移、火山喷发的火山灰进入大气层形成的阳伞效应等等，这些都会影响地球上接收到的太阳辐射变化。&lt;strong&gt;辐射平衡&lt;/strong&gt;是地球气候系统形成的重要基础，辐射平衡的改变也就改变了气候，冷暖、干湿。终极一切奥秘，肯定都在天文宇宙之中。人类太渺小了，太短暂了，太微不足道了……&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习与环境气象：空气质量预报：4业务生产</title>
      <link>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A4%E4%B8%9A%E5%8A%A1%E7%94%9F%E4%BA%A7/</link>
      <pubDate>Fri, 21 Jul 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A4%E4%B8%9A%E5%8A%A1%E7%94%9F%E4%BA%A7/</guid>
      <description>&lt;p&gt;建立模型和方案的最终目的是能够应用于未来每天的实际预测业务生产中。因此，&lt;strong&gt;很重要的一点&lt;/strong&gt;必须记住，那就是未来可用于预测的特征必须能够和之前建模训练时采用的特征保持完全一致。也就是说，如果建模训练时采用的特征在后来不可获取了，那么模型也就无法应用到实际的日常生产中去了。所以，在建模训练的时候就必须关注这个问题。在条件不能再满足的情况下，重新建模训练甚至重新做特征工程都是有可能的。&lt;/p&gt;

&lt;p&gt;模型方案确定可行之后，要做的就是业务化部署了。这一部份工作就是对预报方案的封装，然后设定定时自动化任务并记录运行日志。如果是在linux系统上，crontab较为常用。如果实在windows上，也可以配置任务计划。此外，python也有软件包（常用apscheduler）可以做定时任务配置。&lt;/p&gt;

&lt;p&gt;既然是业务化生产，就要监控各种可能出现的异常情况，用python的logging模块可以很好的承担这个工作。对于各种异常情况造成的数据缺失情况，也应有相应的处理措施，具体问题需要具体分析，以保障日常生产。&lt;/p&gt;

&lt;p&gt;作为完整的业务流程，每天的预报结果通常以文件存储或写入数据库，这些方法一并封装在程序中就可以了。用python做这些工作通常都是非常简单的。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习与环境气象：空气质量预报：3建模、训练和评估</title>
      <link>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A3%E5%BB%BA%E6%A8%A1%E3%80%81%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0/</link>
      <pubDate>Thu, 20 Jul 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A3%E5%BB%BA%E6%A8%A1%E3%80%81%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0/</guid>
      <description>&lt;p&gt;Python有很多机器学习方面的软件包（scikit-learn、TensorFlow、Keras等），对于我们应用于构建和训练模型都是非常方便的，官方文档就是很好的示例。工具都有了，剩下的问题就是具体问题具体分析，从理论角度&lt;strong&gt;选取合适的模型和方案&lt;/strong&gt;，然后尝试训练和评估模型的表现，最后获得可以用于日常生产的模型和方案。&lt;/p&gt;

&lt;p&gt;拿我们现在的空气质量预报问题来说，对于某一种污染物或者空气质量指数AQI，其伴随的天气特征是多元的，其关系可以是线性的也可以是非线性的。&lt;strong&gt;从适用的模型上来说&lt;/strong&gt;，人工神经网络模型、基于统计学的广义线性模型、基于决策树的集合方法，都可以尝试。&lt;strong&gt;从适用的方案上来说&lt;/strong&gt;，不同时空（不同地区、不同季节）匹配的条件下，空气质量与天气条件的关系也会不同，因此，对于不同地区、不同季节，应当分别进行建模、训练和评估。&lt;/p&gt;

&lt;p&gt;在实际运用于生产业务的时候，气象上还通常采用&lt;strong&gt;滚动预报&lt;/strong&gt;的方法，比如每天用之前一段时间的数据进行建模并用于未来若干天的预报。&lt;/p&gt;

&lt;p&gt;预报效果怎么样，模型和方案的评估就很重要。评估的对象取决于所关注的目标。一般，通过统计分析正确率、漏报率、空报率、均方根误差（RMSE）、平均绝对误差（MAE）等特征，可以做出较为全面的评估。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习与环境气象：空气质量预报：2数据处理</title>
      <link>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A2%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</link>
      <pubDate>Wed, 19 Jul 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A2%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</guid>
      <description>&lt;p&gt;检查完数据质量问题，就要对发现的必要问题进行处理了。其中最重要的问题就是&lt;strong&gt;数据缺失或异常&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;对于发现的数据缺失或异常，如果其样本量不大，而且可以利用时间和空间关系进行&lt;strong&gt;插补&lt;/strong&gt;的话，那么还算比较好的情况，也是我们比较期望出现并能解决的情况。如果数据缺失或异常的样本量比较大，或者其它原因导致无法进行合理插补以弥补此类数据质量问题，那么我们很可能要被迫放弃这些数据，其后果是可能导致后续工作受到严重限制甚至无法进行，这是我们最头疼的清醒，但愿不要出现。&lt;/p&gt;

&lt;p&gt;在处理完数据质量问题之后，我们要做的工作就是对数据进行必要的转换，也就是进一步信息化，以便后续的模型构建和训练工作。因此，这一步要做的工作是非常重要的，如果没有做好&lt;strong&gt;必要的、合理的&lt;/strong&gt;数据转换工作，后续工作可能就无法取得比较理想的结果。&lt;/p&gt;

&lt;p&gt;以天气预报信息为例，风向（东南西北风）可以转换为数字标号或者360度，风力（微风、3-4级）可以转换为数字标号或者特定风速数值m/s，天气现象（晴、多云、阴、雨雪等等）则通常转换为0、1标签（0表示没出现，1表示有出现）。这些是原始信息的基本转换工作。&lt;/p&gt;

&lt;p&gt;还有一部分转换工作在于从专业角度分析影响结果的哪些&lt;strong&gt;特征&lt;/strong&gt;可以被提取出来。以我们现在的空气质量预报来说，天气条件不仅仅是当时的数值，&lt;strong&gt;天气的变化信息&lt;/strong&gt;反而更为重要，这就需要我们把他们从原始数据中萃取出来。比如，24小时的气温和气压变化、一天当中气温的日较差、风向的变化等等，它们对于空气质量来说，都可能是重要的影响的因素。换句话说，空气质量这样或者那样，伴随着的是这样或者那样的天气特征。因此，这一部分工作，就是机器学习中所说的&lt;strong&gt;特征工程&lt;/strong&gt;。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习与环境气象：空气质量预报：1爬取数据</title>
      <link>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A1%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Tue, 18 Jul 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A1%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE/</guid>
      <description>&lt;p&gt;从网上爬取数据是获取数据的重要途径，相比于文本类型的数据（比如用户评论），气象和环境方面的数据多以数字为主，爬取似乎简单一些。但是对于我们做研究来说，数据的完整性就比较重要，如果数据质量不好，会比较让人头疼。空气质量监测数据的数据源很多，官方的是环保总站公布的数据，还有很多第三方提供数据接口，可以仔细看看，选择&lt;strong&gt;数据质量好的、来源稳定的&lt;/strong&gt;。天气数据来源也很多，比如中国天气网官网，另外也有很多第三方数据源可以获取（他们的数据大多也来自气象部门，差别不大），也要选取&lt;strong&gt;数据质量好的、来源稳定的&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;对于我们的问题来说，有一点需要&lt;strong&gt;特别注意&lt;/strong&gt;，那就是无论气象数据还是空气质量数据，不光要有每天实时更新的数据（天气预报数据和空气质量实况数据），还需要有历史实况数据。还好这样的数据源都是可以找到的。&lt;/p&gt;

&lt;p&gt;数据爬取之前需要做的是仔细分析一下数据来源，这可能会大大减少爬取数据的工作量。比如，如果同一数据源，即有电脑浏览器页面又有手机浏览器页面，那么手机浏览器页面一般比较简单，容易分析。再比如，对于Ajax加载的数据，利用浏览器自带的工具仔细分析一下数据来源的请求，比调上来就调用Selenium要简单得多。&lt;/p&gt;

&lt;p&gt;Python的话，我们直接采用requests这个工具就可以很轻松完成数据爬取的工作了。&lt;/p&gt;

&lt;p&gt;爬下来的数据格式可能不是我们想要的，那么接下来要做的工作就是必要的格式转换。通常，json和csv都是很不错的格式。如果数据量比较大，还可以考虑用数据库来存储，对于简单一些的应用，sqlite就够了。对于数据处理，python的numpy和pandas通常都是很好用的工具。对于数据的预览（人工检视），用matplotlib画个曲线图出来看看是很直观的。另外，还可以从数据的基本统计信息来检查数据质量（特别是数据缺失和异常），比如最大最小值、平均值、样本数、方差等等，对于排查数据质量问题都有帮助。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>机器学习与环境气象：空气质量预报：0方案设计</title>
      <link>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A0%E6%96%B9%E6%A1%88%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Mon, 17 Jul 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E7%8E%AF%E5%A2%83%E6%B0%94%E8%B1%A1%EF%BC%9A%E7%A9%BA%E6%B0%94%E8%B4%A8%E9%87%8F%E9%A2%84%E6%8A%A5%EF%BC%9A0%E6%96%B9%E6%A1%88%E8%AE%BE%E8%AE%A1/</guid>
      <description>&lt;p&gt;现在到处都是人工智能和机器学习的新闻，气象也在凑热闹。看热闹的不怕事大，我们也来围观。&lt;/p&gt;

&lt;p&gt;话说回来，其实，机器学习在气象预报方面的能力是不能跟在图像语音识别等方面相比的。道理很简单，图像语音识别这类问题是比较明确的，&lt;strong&gt;不确定性&lt;/strong&gt;很小，人工的识别准确率很高，机器学好了可以更高更快。气象预报就不一样了，不管是人工基于天气学原理的分析还是计算机基于数学物理模型的的预报，准确率的水平都不能跟图像语音识别这类问题相比，不确定性还是太高了。如果说机器学习在气象上有好的应用方面的话，一定是那些不确定性相对比较小的地方（比如基于雷达回波的短临天气预报），或者数学物理模型还无法描述的地方（比如气候预测），也许能够取得比常规方法好一些的结果。能好多少，不知道，但是不会有本质的飞跃。&lt;/p&gt;

&lt;p&gt;这里我们做一点简单的工作，基于机器学习的方法来试着做做空气质量预报。&lt;strong&gt;总体思路&lt;/strong&gt;是这样的：从网上获取空气质量的历史数据和实时更新的监测数据，从网上获取天气实况的历史数据和实时更新的天气预报数据，用机器学习的方法发掘空气质量和气象条件之间的关联，构建基于气象因素的空气质量预报模型，最后用天气预报来预测未来同期的空气质量。所以我们的工作分为以下几个部分：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;爬取空气质量和天气数据。&lt;/li&gt;
&lt;li&gt;数据信息的转换（为构建和训练模型做准备）。&lt;/li&gt;
&lt;li&gt;尝试构建和训练模型，并进行评估。&lt;/li&gt;
&lt;li&gt;实际运用于日常预报工作，实现自动化运作。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s GO！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>K-means聚类分析应用示例</title>
      <link>https://wormone.github.io/notebook/post/K-means%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/</link>
      <pubDate>Tue, 13 Jun 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/K-means%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90%E5%BA%94%E7%94%A8%E7%A4%BA%E4%BE%8B/</guid>
      <description>&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

&#39;&#39;&#39;KMeans 聚类分析样例程序

关于样本和样本（点和点）之间距离的算法：
http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html#sklearn.metrics.pairwise.pairwise_distances

用 silhouette 评估 KMeans 聚类分析，画图，确定分类数目：
http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py

在 KMeans 之前先用主成分分析（PCA）降维：
http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py

&#39;&#39;&#39;


import numpy as np
import matplotlib.cm as cm
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances, silhouette_score, silhouette_samples


# 载入样例数据
from sklearn import datasets
dataset = datasets.load_iris()
X = dataset.data
# y = dataset.target  # 这是实际分类标签。由于 KMeans 是非监督（自动）分类算法，因此不用学习这个真实分类。
n_samples, n_features = X.shape
print(&#39;n_samples = {}, n_features = {}&#39;.format(n_samples, n_features))


&amp;quot;&amp;quot;&amp;quot;
    需要特别注意的是：这里的样例数据比较简单，每个样本的不同特征的量纲和数量级相同，但是我们要面对的问题可能不是这样：
    一天（看作一个样本、一个点）的特征有风速风向、最高最低气温、湿度、气压，及其24小时变化值等，
    不同要素的量纲和数量级不同，因此最好在进行 KMeans 聚类分析之前，先对每个要素进行标注化或归一化处理。
    归一化即 (x-min(x))/(max(x)-min(x))，把 x 转变为 [0, 1] 区间内。
    标准化即 (x-mean(x))/std(x)，结果有正有负，标准差变为 1。
&amp;quot;&amp;quot;&amp;quot;


&amp;quot;&amp;quot;&amp;quot; 
    以下两个 KMeans 样例程序，都是把示例数据 X 分成 3 类 
&amp;quot;&amp;quot;&amp;quot;


def test_KMeans_default(X=X, n_clusters=3):
    # 直接采用 KMeans 进行聚类分析，默认距离计算方法是 euclidean，即欧几里得距离
    kmeans_model = KMeans(n_clusters=n_clusters, random_state=1).fit(X)
    cluster_labels = kmeans_model.labels_
    silhouette_avg = silhouette_score(X, cluster_labels, metric=&#39;euclidean&#39;)
    sample_silhouette_values = silhouette_samples(X, cluster_labels, metric=&#39;euclidean&#39;)
    # print sample_silhouette_values
    print(&#39;Average silhouette value: {}&#39;.format(silhouette_avg))


def test_KMeans_using_custom_distance(X=X, n_clusters=3, metric=&#39;correlation&#39;):
    # 先按照某种距离计算方法计算距离矩阵 D，如采用 correlation 算法，即 1 减去皮尔逊相关系数，这种算法在我们需要解决的问题中更常用
    D = pairwise_distances(X, metric=metric)
    clusterer = KMeans(n_clusters=n_clusters, random_state=1, precompute_distances=True)
    cluster_labels = clusterer.fit_predict(D)
    silhouette_avg = silhouette_score(D, cluster_labels, metric=&#39;precomputed&#39;)
    sample_silhouette_values = silhouette_samples(D, cluster_labels, metric=&#39;precomputed&#39;)
    # print sample_silhouette_values
    print(&#39;Average silhouette value: {}&#39;.format(silhouette_avg))

    &amp;quot;&amp;quot;&amp;quot;
        以下绘图程序复制粘贴自官网示例，样例数据 X 只有 4 个特征，因此右侧的图能看出分类。
        对于高维度的气象数据而言，例如，一天（看作一个样本、一个点）的特征有风速风向、最高最低气温、湿度、气压，及其24小时变化值等，
        因此，无法在简单的二维平面显示他们的分类：画出来的点，虽然属于不同类别，但是都会叠在一起，分不出来。
    &amp;quot;&amp;quot;&amp;quot;
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title(&amp;quot;The silhouette plot for the various clusters.&amp;quot;)
    ax1.set_xlabel(&amp;quot;The silhouette coefficient values&amp;quot;)
    ax1.set_ylabel(&amp;quot;Cluster label&amp;quot;)

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color=&amp;quot;red&amp;quot;, linestyle=&amp;quot;--&amp;quot;)

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # # 2nd Plot showing the actual clusters formed
    # colors = cm.spectral(cluster_labels.astype(float) / n_clusters)
    # ax2.scatter(X[:, 1], X[:, 2], marker=&#39;.&#39;, s=30, lw=0, alpha=0.7,
    #             c=colors)

    # # Labeling the clusters
    # centers = clusterer.cluster_centers_
    # # Draw white circles at cluster centers
    # ax2.scatter(centers[:, 0], centers[:, 1],
    #             marker=&#39;o&#39;, c=&amp;quot;white&amp;quot;, alpha=1, s=200)

    # for i, c in enumerate(centers):
    #     ax2.scatter(c[0], c[1], marker=&#39;$%d$&#39; % i, alpha=1, s=50)

    # ax2.set_title(&amp;quot;The visualization of the clustered data.&amp;quot;)
    # ax2.set_xlabel(&amp;quot;Feature space for the 1st feature&amp;quot;)
    # ax2.set_ylabel(&amp;quot;Feature space for the 2nd feature&amp;quot;)

    # plt.suptitle((&amp;quot;Silhouette analysis for KMeans clustering on sample data &amp;quot;
    #               &amp;quot;with n_clusters = %d&amp;quot; % n_clusters),
    #              fontsize=14, fontweight=&#39;bold&#39;)

    plt.show()


if __name__ == &#39;__main__&#39;:
    # test_KMeans_using_custom_distance(metric=&#39;euclidean&#39;)
    test_KMeans_using_custom_distance()

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>气象数据查询接口设计的简要方案</title>
      <link>https://wormone.github.io/notebook/post/%E6%B0%94%E8%B1%A1%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1%E7%9A%84%E7%AE%80%E8%A6%81%E6%96%B9%E6%A1%88/</link>
      <pubDate>Thu, 13 Apr 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%B0%94%E8%B1%A1%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1%E7%9A%84%E7%AE%80%E8%A6%81%E6%96%B9%E6%A1%88/</guid>
      <description>&lt;p&gt;这里我们指的是存储在数据库里的&lt;strong&gt;站点气象数据&lt;/strong&gt;，存储在文件中的格点数据是另一回事了。&lt;/p&gt;

&lt;p&gt;气象数据的特点是包括时间、地点、层次、要素等&lt;strong&gt;多个维度&lt;/strong&gt;，通常以特定文本格式存储的数据不便于数据的提取和查询，尤其当这种提取和查询经常发生的时候。因此，导入关系型数据库，然后编写查询方法，对用户提供查询服务就是非常必要的了。&lt;/p&gt;

&lt;p&gt;通常，数据源的格式是固定的，而且随着时间的延伸，数据会不断更新，那么编写一套数据读取和写入数据库的程序就会方便后续的数据更新工作了。&lt;/p&gt;

&lt;p&gt;python操作数据库有&lt;strong&gt;两种方式&lt;/strong&gt;，一种是不同数据库采用不同的软件包（如对于MS SQL可以使用pymssql，对于MySQL可以使用pymysql，对于sqlite可以使用sqlite3），一种是基于对象-关系映射模型（ORM）的软件包（如SQLAlchemy）。两种方式都是比较容易的，官方文档都有很好的示例，参照按部就班写就可以了。使用SQLAlchemy的好处是可以更容易的切换不同数据库。&lt;/p&gt;

&lt;p&gt;数据库表的设计应当看数据的具体情况，具体分析，没有太多可说的。当写入和读取数据库数据的方法写好之后，就是后续的调用和封装工作了。读取的方法有哪些，取决于查询的需求有哪些，一一对应就可以了。通常，查询都包括时间范围、空间范围、要素名称、统计方式（如平均值、累计值、最大最小值）等方面。&lt;/p&gt;

&lt;p&gt;接下来就是对外提供数据查询服务了，可能直接被用户调用，也可能被其它外部程序调用。数据查询接口一般只涉及到数据的读取，很少涉及到数据的写入，因此来说功能比较单一，实现起来也比较容易。对于上述写好的读取方法，只需要利用HTTP服务封装一下就可以了。我们采用&lt;strong&gt;Flask&lt;/strong&gt;或&lt;strong&gt;Tornado&lt;/strong&gt;框架就可以很轻松完成任务了，其中需要注意的基本问题包括：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;url的设计应符合约定俗成的规范，清晰明了。&lt;/li&gt;
&lt;li&gt;只编写和允许必要的请求方法，禁止不必要的请求。&lt;/li&gt;
&lt;li&gt;接口说明文档应编写好并放在一个类似/api/help的url里以便用户查阅。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;进一步，可能遇到的问题包括：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;如果有查询不同结果格式的需求，应在查询结果之后增加一个统一的格式处理方法。&lt;/li&gt;
&lt;li&gt;如果有区分用户权限的功能需求，应在查询方法之前增加一个权限管理模块。&lt;/li&gt;
&lt;li&gt;如果有提高服务的性能的需求，应相应的从硬件配置、缓方式、多任务并行等方面解决。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>气象数据处理和绘图常用的python软件包</title>
      <link>https://wormone.github.io/notebook/post/%E6%B0%94%E8%B1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E7%BB%98%E5%9B%BE%E5%B8%B8%E7%94%A8%E7%9A%84python%E8%BD%AF%E4%BB%B6%E5%8C%85/</link>
      <pubDate>Fri, 13 Jan 2017 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E6%B0%94%E8%B1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%92%8C%E7%BB%98%E5%9B%BE%E5%B8%B8%E7%94%A8%E7%9A%84python%E8%BD%AF%E4%BB%B6%E5%8C%85/</guid>
      <description>

&lt;h3 id=&#34;1-基本问题&#34;&gt;1. 基本问题&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;- datetime, canlendar：时间和日期处理
- re：正则处理
- glob：文件名匹配
- os, shutil：系统路径等操作
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;2-特定格式&#34;&gt;2. 特定格式&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;- netCDF4：netCDF数据读写
- pygrib：grib数据读写
- struct：二进制数据读写
- gdal：geotiff等数据读写
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;3-计算分析&#34;&gt;3. 计算分析&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;- numpy, scipy：科学计算
- pandas：数据结构框架
- scikit-learn：统计学和机器学习模型
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;4-绘图&#34;&gt;4. 绘图&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;- matplotlib, basemap：几乎所有类别的气象图形
- seaborn：更美观一些的图形
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;5-业务自动化&#34;&gt;5. 业务自动化&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;- apscheduler：定时任务管理
- multiprocessing：多进程并行计算
- celery：分布式并行框架
- logging：日志记录
- pyWRF：WRF管理
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>基于leaflet.js的气象数据可视化方案</title>
      <link>https://wormone.github.io/notebook/post/%E5%9F%BA%E4%BA%8Eleaflet.js%E7%9A%84%E6%B0%94%E8%B1%A1%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E6%96%B9%E6%A1%88/</link>
      <pubDate>Sun, 13 Nov 2016 19:21:15 +0800</pubDate>
      
      <guid>https://wormone.github.io/notebook/post/%E5%9F%BA%E4%BA%8Eleaflet.js%E7%9A%84%E6%B0%94%E8%B1%A1%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E6%96%B9%E6%A1%88/</guid>
      <description>&lt;p&gt;气象数据是多维度的数据，包括时间、空间、要素名等。空间属性是气象数据的基本属性。&lt;/p&gt;

&lt;p&gt;气象数据的可视化离不开空间分布的展示。单点数据以地理坐标为基础，通常在地图上以数据、符号、颜色等标识。例如风向通常以风杆或者带有方向箭头的小图标标识。网格化数据通常以等值线（contour）、填充色（contourf）或色块（imagesc，pcolor）标识。&lt;/p&gt;

&lt;p&gt;上述两类数据均可以用&lt;strong&gt;geojson&lt;/strong&gt;的格式组织起来交由&lt;strong&gt;leaflet.js&lt;/strong&gt;在页面上展示。&lt;/p&gt;

&lt;p&gt;单点的数据很简单。网格化数据是以等值线分析为基础的，这个工作可以交由我们熟悉的&lt;strong&gt;matplotlib&lt;/strong&gt;来完成，其输出结果再由&lt;strong&gt;geojsoncontour&lt;/strong&gt;这个第三方软件包转换为geojson格式即可（该软件包在github上可以找到，其中的一个小bug已被我们修复）。&lt;/p&gt;

&lt;p&gt;leaflet.js提供了丰富的方法用户展示基于GIS的信息，并提供了很多交互操作的功能，官方文档都有很好的示例。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>